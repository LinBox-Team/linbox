\section{Improving \linbox matrix multiplication}\label{sec:matmul}
%
% Efficient matrix multiplication is key to \linbox library.
%
\subsection{Plug-in structure}
%
We propose the following \emph{design pattern} (the closest design pattern to our
knowledge is the \emph{strategy} one, see also \cite[Fig 2.]{Cung:2006:TC}).
The main advantage of this design pattern is that the modules always call back
the controller so that the best choice is always chosen.  Besides modules can
be easily added as \emph{plug-ins}.
%
\input{fig_control}
%
An analogy can be drawn with dynamic
systems---once the controller sends a correction to the system, it receives
back a new measure that allows for a new correction.
%
%
%
\par
%
For instance, we can write (\Cref{fig:seuil}) the standard cascade algorithms
(see \cite{Dumas:2008:Flas}) in that model. Cascade algorithms are used to combine
several algorithms that are switched using thresholds, ensuring better
efficiency than any of the algorithms within used separately.
%
% \par
%
%
This method allows for the reuse of modules and ensures efficiency.
It is then possible to adapt to the architecture, the available modules,
the resources. The only limitation is that the choice of the module
should be done fast.
%
\input{fig_cascade}
% \par
%
On top of this design, we have Methods/Helpers that allow (preferred) selection
of algorithms and cut short in the strategy selection.
%
% \par
%
% \danger timing old fgemm/plugin fgemm with no noticeable change ?
%
\par
%
This infrastructure also invites to modularise the code. For instance,
\fflasffpack has seen major  modularization (addition, scaling,
reduction,\ldots) Not only it allows to write code with hardly more line than
it takes for pseudo-code listings in \cite{Boyer:2009:sched} (compared to
$\approx 2.5\times$ on some routines before) but also it automatically brings
performance, because we can the separately improve on these modules and can
switch easier between them. Also, this reduces the lines of code, hence the
probability for bugs, and eases the tracing/tracking of bugs, allows for more
unit tests. Modularising the code comes at almost no cost because we may add
$O(1)$  operations that 1) don't cost much compared to $O(n^2)$ or more
complexity of the modules; 2) allow early decisions and terminations (\eg by
testing against $0$, $\pm 1$, checking leading dimensions and increments); 3)
allow better code (AVX, SSE, copy--cache friendly operation--copy back,
representation switching,\ldots)
%
\subsection{New algorithms/infrastructure}
%
We introduce now several new algorithms that improve on matrix multiplication
in various ways: reducing memory consumption, introducing new efficient
algorithms, using graphics capabilities, generalizing the BLAS to integer
routines.
%
\paragraph{New algorithms: low memory}
%
The routine \fgemm in \fflas uses the classic schedules for the multiplication and
the product with accumulation (\cf \cite{Boyer:2009:sched}), but we also
implement the lower memory routines therein.
%
% \par
%
The difficulty consists in using the part of the memory contained in a
sub-matrix of the original matrix. It is two-fold. -- First we have to use some
part of a memory that has already been allocated to the input matrices,
therefore we cannot free and reallocate part of it. -- Second, several of these
algorithms are meant for square matrices.
%
% \par
%
% XXX \danger table comparing speeds
%
\paragraph{New algorithms: Bini.}
%
In \cite{BD:2014:Bini}, we use Bini's approximate matrix multiplication formula
to derive a new algorithms that is more efficient that the Strassen--Winograd
implementation in \fgemm by $\approx 5-10\%$ on sizes \num{1500}--\num{3000}.
This is a cascade of Bini's algorithm and Strassen--Winograd algorithm and/or
the naÃ¯ve algorithm (using BLAS). The idea is to analyse precisely the error
term in the approximate formula and make it vanish.
%
\paragraph{Integer BLAS.}
%
In order to provide fast matrix multiplication with multiprecision integers, we
rely on multi-modular approach through the chinese remainder theorem. Our
approach is to reduce as much as possible to \fgemm. Despite, the existence of
fast multimodular reduction (resp.\ reconstruction) algorithm
\cite{VonzurGathen:1999:MCA}, the naive quadratic approach can be reduced to
\fgemm which makes it more efficient into practice.  Note that providing
optimized fast multimodular reduction remains challenging. This code is
directly integrated  into \fflas.

%
\paragraph{Polynomial Matrix Multiplication over small prime fields.}
%
The situation is similar to integer matrices since one can use
evaluation/interpolation techniques through DFT transforms. However, the
optimized Fast Fourier Transform of \cite{Harvey:2014}  makes fast evaluation
(resp.\ interpolation) competitive into practice. We thus rely on this scheme
together with \fgemm for pointwise matrix multiplications. One can find some
benchmark of our code in \cite{GioLeb14}.

\paragraph{OpenCL}
%
\danger dave
\paragraph{Sparse Matrix--Vector Multiplication}
%
Sparse matrices are usually problematic because the notion of \emph{sparsity}
is too general \emph{vs.} the specificity of real world sparse matrices: the
algorithms have to adapt to the shape of the sparse matrices ---which is not
really the case for the dense case.
%
Getting the best performance for an sparse matrix as an \applin is a
challenging task. There is a huge literature on \spmv (Sparse Matrix Vector
multiplication) and on sparse matrix formats, some of which are becoming
standard (COO, CSR, BCSR, SKY,\ldots).  In \cite{Boyer:2010:spmv} we developed
some techniques to improve the \spmv operation in \linbox.
Just like the \textsc{Blas} numerical
routines, we would also like to take advantage of existing high performance
numerical libraries (come back to this later).
%
\par
%
The addition of standard matrix format is driven by the availability of
numerical routines and the expectation of better performance in the \spmv
operation.
The issue in the \spmv operation is taking advantage of existing numerical
routines. There is a Sparse BLAS specification, but it is not widely
implemented (Intel's MKL does, there is a cuSPARSE library in the Nvidia Cuda
toolkit). Contrary to dense BLAS representation, there is no way of viewing a
sparse submatrix in most of the standard formats, so we cannot apply the same
techniques as those in \fflas for $Z/pZ$. However, we develop hybrid structures
that represent a matrix $A$ as a sum $A_1 + A_2 + \cdots + A_k$. For instance,
the number of non zeros in each row of each $A_i$ will not overflow a dot
product and numerical sparse BLAS can be used. Besides, the format of each
submatrix is independent and we can create some ELL+CSR format for instance.
Besides, sparse matrices  from many domains have only $\pm1$ as non zero
entries and we take advantage of that (no multiplication, accumulation further
delayed, no need to store the data).
%
\par
%
Contrary to the BLAS case, the apply using the transpose of $A$ can be very
inefficient compared to the apply on $A$. In this case, we create a helper
function that (if memory allows) just stores $A^\top$ in an optimised hybrid
format too. Setting up the hybrid representation of $A$ usually costs the order
of $5$to $10$ non optimised \spmv operations: the conversion is performed only
for larger matrices that will be used repeatedly for the \spmv operation and
remains constant.
% They are however not very widespread, for instance sparse blas in
% intel mkl (only). Metis for graph partitionning.  zero-one matrices are special
% (no numerical routines)
% Helper structure to store a matrix as a sum of structures (HYB), possibly the
% transpose. Memory.  Reduce inits on Z/pZ.
\paragraph{Using conversions}
\begin{itemize}
	\item
 double->float
	\item
 using flint for integer matmul is faster, even with conversion. Need better CRA implementation (but with the plugins, we can do without our faulty code and just use flint).
	\item
 implementation of Toom-Cook for GF(q)
	\item
 when does spmv choose to optimise ?
	\item
 transition to benchmarking
\end{itemize}
%
\danger dave some words on the Domain architecture that you support and why it is best than functions (for instance for mul or apply) ?
