As I understand it, Field.cpp, Field2.cpp, Field3.cpp, and FieldZZ.cpp 
are files on which we ran some timing tests while we were at Grenoble.

In any case, they are files on which Austin and I got surprising 
timings yesterday.  And now I have done an experiment today which has 
significantly different results (conclusions are at the end):

Procedure:

Let home denote eecis.udel.edu:/usr/local/algebra/linbox/.
I copied the files abouve mentioned, and supporting files, from
home/Matrix/Field to home/Matrix/Fieldtmp.

1.
I modified Field2 and Field3 to time the setup phase, a change which
I believe is insignificant.

2.
I modified Field2 and Field3 to report the dotprod after each innerproduct,
a change which I believe is VERY significant, see conclusion below.  

3.
I compiled using g++ version 2.8.1, and option -O5.
FieldZZ.cpp was left out of this study, since at link phase there were
missing objects and I didn't know what to do about that.

4. I ran each with the same data: vector of length 100001, innerproduct
computed 10 times over. 
(But different embedded primes though - 101, 7, 7 if I remember right.)
I got the following 3 outputs.  The use of the shell time command 
accounts for the last line of each output.  I used this as a sanity 
check of givtimer (it checks well).  These runs were on an otherwise idle 
machine, a SUN4U/250 Ultra-30/250 with SPECint 10.0.


Results:

taygeta.cis.udel.edu: time Field.out 1000001 10
Time
user time: 7.22
sys. time: 7.22
real time: 7.22

-24

Time
user time: 7.19
sys. time: 7.19
real time: 7.19

-24
14.540u 0.060s 0:28.76 50.7%    0+0k 0+0io 0pf+0w
taygeta.cis.udel.edu: time Field2.out 1000001 10
0
3
Time setup
user time: 0.11
sys. time: 0.11
real time: 0.11

Time AbsField 
user time: 7.24
sys. time: 7.24
real time: 7.24

-6

Time C array direct 
user time: 7.22
sys. time: 7.22
real time: 7.22

-6

14.500u 0.080s 0:28.66 50.8%    0+0k 0+0io 0pf+0w
taygeta.cis.udel.edu: time Field3.out 1000001 10
0
3
Time setup
user time: 0.12
sys. time: 0.12
real time: 0.12

Time AbsField
user time: 7.22
sys. time: 7.22
real time: 7.22

-6

Time C array direct
user time: 7.23
sys. time: 7.23
real time: 7.23

-6

14.520u 0.060s 0:28.90 50.4%    0+0k 0+0io 0pf+0w

5. As a check on inlining, I compiled Field3 again, this time specifying
that inline's be honored.

taygeta.cis.udel.edu: g++ -O5 -all -inline Field3.cpp 
taygeta.cis.udel.edu: time a.out 1000001 10
0
3
Time setup
user time: 0.11
sys. time: 0.11
real time: 0.12

Time AbsField
user time: 7.22
sys. time: 7.22
real time: 7.22

-6

Time C array direct
user time: 7.23
sys. time: 7.23
real time: 7.23

-6

14.520u 0.070s 0:28.78 50.6%    0+0k 0+0io 0pf+0w


6. Conclusions

6a.  All times are identical, therefore the differences in implementation
explored in these tests are unimportant in the test situation (a single 
innerproduct).  If I understand right, these tests involved consideration 
of the use of a trait containing a pointer to actual field vs. a trait 
containing the field object, of stl vectors vs. arrays, of functions 
located in class vs. located in parent class, and of arithmetic obtained 
generically vs. direct mod p.

Generalization: Pointer dereferencing is not a cost if a sequence of 
references are to the same object or if a sequence of references are 
to well localized objects.

Remark:  I believe it will be very important to our performance to have
fast (sparse and dense) vector operations over small fields.  These tests
do not really address the locality problems we'll have there.

Belief:  These tests have NOT spoken to the "factor of 25" question.

Conjecture on a nearby topic:  Discrete log arithmetic with it's
table lookup performs quite well on these sparcs.  The array lookup is 
quite "random" and can be expected to involve a cache miss each time.  
The relatively good performance is probably due to the badness of sparc 
integer mult and % on the one hand, and to the goodness of the handling 
of these lookups in the instruction pipeline on the other hand.

6b.  As Mark pointed out, user inlining of small functions is unnecessary.
The compiler will do it anyway.

6c. It is important in timing tests, perhaps especially with the 
optimization level set high, that the result of each timed computation is 
USED, eg. in a print statement.  Otherwise the compiler may skip the 
computation entirely or skip large parts of it.  Belief: this accounts
for the dramatic differences in times in our earlier runs of these tests.

6d. Givtimer seems to be giving good user time, but incorrect sys and real 
times.  And givtimer code looks like it could stand a little cleaning up.

6d'. Just a remark, fyi: a few years ago, working with Linda, I found that 
they had a very accurate, fine grained clock.  I DON'T think we have need of 
this, nor do I know how one gets access to a clock that ticks more than
at 1/100 sec, but they clearly had something like a microsecond clock.

-dave
