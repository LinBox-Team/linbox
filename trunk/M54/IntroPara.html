<html><head><title>Introduction to Parallel Programming</title>

<script language="javascript">

function PrintFooter() {
  document.write("<P><HR><FONT SIZE=2><I><BR> \
    <A HREF=\"/\">Maui High Performance Computing Center</A> \
    All rights reserved.<BR>");
  document.write(document.location,"<BR>");
  document.write("Last Modified: ",document.lastModified," ");
  document.write("<A HREF=mailto:webmaster@mhpcc.edu>webmaster@mhpcc.edu</A>");
  document.write("</I></FONT><P>");
  defaultStatus = document.location;
  }

function popUp(url) {
  HelperWindow=window.open('', '', 'toolbar=no,menubar=no,location=no,  \
    directories=no,status=no,scrollbars=yes,resizable=yes,copyhistory=no,  \
    width=800,height=400');
  HelperWindow.location=url;
  HelperWindow.focus();
}

</script></head>



<body bgcolor="white"> 
<basefont size="3">            <!-- default font size -->

<a name="top">   </a>
<center>
<br>
<table border="0" cellspacing="0" cellpadding="5" width="100%">
<tbody><tr>
<td bgcolor="navy"><font face="helvetica" size="+2" color="white">
<b>SP Parallel Programming Workshop
</b></font></td></tr><tr>
<td bgcolor="#cccccc" align="right"><font face="helvetica" size="+2" color="red">
<b>p a r a l l e l     p r o g r a m m i n g    
i n t r o d u c t i o n
</b></font></td></tr></tbody></table>
</center>

<br><br><br>

<a name="#TOC">   </a>
<font size="+2" face="helvetica"><b>
Table of Contents
</b></font>
<ol>
<li><a href="#what%20is%20parallelism">
    Overview </a>
    <ol>
    <li><a href="#what%20is%20parallelism">
         What is Parallelism? </a>
    </li><li><a href="#sequential%20programming">
         Sequential Programming </a>
    </li><li><a href="#need%20for%20faster%20machines">
         The Need for Faster Machines </a>
    </li><li><a href="#parallel%20computing">
         Parallel Computing </a>
    </li><li><a href="#parallel%20programming">
         Parallel Programming Overview</a>
    </li></ol>
  
</li><li><a href="#architecture%20taxonomy">
     Architecture Taxonomy</a>
     <ol>
     <li><a href="#sisd%20model">
          SISD Model </a>
     </li><li><a href="#simd%20model">
          SIMD Model </a>
     </li><li><a href="#mimd%20model">
          MIMD Model </a>
     </li></ol>

</li><li><a href="#memory%20architectures">
     Memory Architectures </a>
     <ol>
     <li><a href="#shared%20memory">
          Shared Memory </a>
     </li><li><a href="#distributed%20memory">
          Distributed Memory </a>
     </li><li><a href="#memory-processor%20arrangements">
          Memory-Processor Arrangements </a>
     </li></ol>

</li><li><a href="#communications%20network">
     Processor Communication </a>
     <ol>
     <li><a href="#communications%20network">
          Communications Network on the IBM SP </a>
     </li></ol>

</li><li><a href="#paradigms">
     Parallel Programming Paradigms</a>
     <ol>
     <li><a href="#paradigms">
          Various Methods </a>
     </li><li><a href="#MP%20Definition">
          Message Passing  </a>
     </li><li><a href="#DP%20Definition">
          Data Parallel </a>
     </li><li><a href="#Implementations"> Implementations </a>
          <ul>
          <li><a href="#MPI%20Overview">Message Passing:
              Message Passing Interface (MPI)</a>
          </li><li><a href="#HPF%20Overview">Data Parallel:
              F90 / High Performance Fortran (HPF) </a>
          </li></ul>
     </li></ol>

</li><li><a href="#steps%20for%20creating%20parallel%20program">
     Steps for Creating a Parallel Program </a>
     <ol>
     <li><a href="#decomposing%20the%20program">
          Decomposing the Program </a>
     </li><li><a href="#Communication">
          Communication </a>
         <ol>
         <li><a href="#Comm%20Point%20to%20Point">
              Point to Point </a>
         </li><li><a href="#Comm%20One%20to%20All%20B">
              One to All Broadcast </a>
         </li><li><a href="#Comm%20All%20to%20All%20B">
              All to All Broadcast </a>
         </li><li><a href="#One%20to%20All%20P">
              One to All Personalized </a>
         </li><li><a href="#Comm%20All%20to%20All%20P">
              All to all Personalized </a>
         </li><li><a href="#Comm%20Shifts">
              Shifts </a>
         </li><li><a href="#Comm%20Collective">
              Collective Computation </a>
         </li></ol>
     </li></ol>

</li><li><a href="#design%20and%20performance%20considerations">
     Design and Performance Considerations</a>
     <ol>
     <li><a href="#amdahl">
          Amdahl's Law </a>
     </li><li><a href="#load%20balancing">
          Load Balancing </a>
     </li><li><a href="#granularity">
          Granularity </a>
     </li><li><a href="#data%20dependency">
          Data Dependency </a>
     </li><li><a href="#deadlock">
          Deadlock </a>
     </li><li><a href="#bandwidth">
          Communication Patterns and Bandwidth </a>
     </li><li><a href="#io%20patterns">
          I/O Patterns</a>
     </li><li><a href="#debugging">
          Debugging </a>
     </li><li><a href="#performance">
          Performance Monitoring and Analysis </a>
     </li></ol>

</li><li><a href="#parallel%20examples">
     Parallel Examples</a>
     <ol>
     <li><a href="#loop%20parallelism">
          Essentials of Loop Parallelism </a>
     </li><li><a href="#pi1">
          Calculating PI </a>
          <ol>
          <li><a href="#pi1">
               Serial Problem Description </a>
          </li><li><a href="#pi2">
               Parallel Solution </a>
          </li></ol>
     </li><li><a href="#array1">
          Calculating Array Elements </a>
          <ol>
          <li><a href="#array1">
               Serial Problem Description </a>
          </li><li><a href="#array2">
               Parallel Solution </a>
          </li><li><a href="#array3">
               Pool of Tasks </a>
          </li><li><a href="#array4">
               Load Balancing and Granularity </a>
          </li></ol>
     </li><li><a href="#heat1">
          Simple Heat Equation </a>
          <ol>
          <li><a href="#heat1">
               Serial Problem Description </a>
          </li><li><a href="#heat2">
               Parallel Solution </a>
          </li><li><a href="#heat3">
               Overlapping Communication and Computation </a>
          </li></ol>
     
</li><li><a href="#application_case_study"> Application Case Study </a> 
</li></ol>
   
</li><li><a href="#References">
     References and More Information</a>
</li></ol>



<a name="what is parallelism"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Overview
</b></font></td><td bgcolor="navy" align="right">
<a href="#TOC"><!-- broken -->
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Table of Contents"></a>
<a href="#sequential%20programming">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Sequential Programming"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
What is Parallelism?
</b></font></td></tr></tbody></table>

<p>
A strategy for performing large, complex tasks faster. 
</p><p>
A large task can either be performed serially, one step following another,
or can be decomposed into smaller tasks to be performed
simultaneously, i.e., in parallel.
</p><p>
Parallelism is done by:
</p><p>
</p><ul>
<li> Breaking up the task into smaller tasks 
<p>
</p></li><li> Assigning the smaller tasks to multiple workers to work on simultaneously
<p>
</p></li><li> Coordinating the workers
</li></ul>
<p>
    Parallel problem solving is common.  Examples: building construction; 
    operating a large organization; automobile manufacturing plant

</p><p>
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/intro_analogy.html#intro" target="W2" onclick="blur()">
<img src="IntroPara_files/bug.gif" width="88" height="60"> The automobile analogy.</a>

<a name="sequential programming"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Overview
</b></font></td><td bgcolor="navy" align="right">
<a href="#what%20is%20parallelism">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to What is Parallelism"></a>
<a href="#need%20for%20faster%20machines">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="The Need for Faster Machines"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Sequential Programming
</b></font></td></tr></tbody></table>

</p><p>
Traditionally, programs have been written for serial computers:
</p><ul>
<li> One instruction executed at a time
<p>
</p></li><li> Using one processor
<p>
</p></li><li> Processing speed dependent on how fast data can move through hardware
<ul>
<p>
</p><li> Speed of Light = 30 cm/nanosecond
</li><li> Limits of Copper Wire = 9 cm/nanosecond
</li></ul>
<p>
</p></li><li> Fastest machines execute approximately 1 instruction in 9-12 billionths
     of a second
</li></ul>



<a name="need for faster machines"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Overview
</b></font></td><td bgcolor="navy" align="right">
<a href="#sequential%20programming">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Sequential Programming"></a>
<a href="#parallel%20computing">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Parallel Computing?"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
The Need for Faster Machines
</b></font></td></tr></tbody></table>

<p>
You might think that one instruction executed in 9 billionths of a 
second would be fast enough.  You'd be wrong.
</p><p>
There are several classes of problems that require faster processing:
</p><p>
</p><ul>
<li> Simulation and Modeling problems:
<p>
</p><ul>
<li> Based on successive approximations
</li><li> More calculations, more precise
</li></ul>
<p>
</p></li><li> Problems dependent on computations / manipulations of large amounts 
     of data
<p>
</p><ul>
<li> Image and Signal Processing
</li><li> Entertainment (Image Rendering)
</li><li> Database and Data Mining
</li><li> Seismic
</li></ul>

<p>
</p></li><li> Grand Challenge Problems:
<p></p><ul>
<li> Climate Modeling
</li><li> Fluid Turbulence
</li><li> Pollution Dispersion
</li><li> Human Genome
</li><li> Ocean Circulation
</li><li> Quantum Chromodynamics
</li><li> Semiconductor Modeling
</li><li> Superconductor Modeling
</li><li> Combustion Systems
</li><li> Vision &amp; Cognition
</li></ul>
</li></ul>



<a name="parallel computing"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Overview
</b></font></td><td bgcolor="navy" align="right">
<a href="#need%20for%20faster%20machines">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Need for Faster Machines?"></a>
<a href="#parallel%20programming">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Parallel Programming Overview"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Parallel Computing</b></font><p>
<font size="+2" face="helvetica"><b>Traditional Supercomputers
</b></font></p></td></tr></tbody></table>

<p>
Technology
</p><ul>
<li> Single processors were created to be as fast as possible.
</li><li> Peak performance was achieved with good memory bandwidth.
</li></ul>
<p>
Benefits
</p><ul>
<li> Supports sequential programming (Which many people understand)
</li><li> 30+ years of compiler and tool development
</li><li> I/O is relatively simple  
</li></ul>
<p>
Limitations
</p><ul>
<li> Single high performance processors are extremely expensive
</li><li> Significant cooling requirements
</li><li> Single processor performance is reaching its asymptotic limit
</li></ul>

<p>
<font size="+2" face="helvetica"><b>
Parallel Supercomputers
</b></font>
</p><p>
Technology
</p><ul>
<li> Applying many smaller cost efficient processors to work on a part of
     the same task
</li><li> Capitalizing on work done in the microprocessor and networking markets
</li></ul>
<p>
Benefits
</p><ul>
<li> Ability to achieve performance and work on problems impossible with 
     traditional computers.
</li><li> Exploit "off the shelf" processors, memory, disks and tape systems.
</li><li> Ability to scale to problem.  
</li><li> Ability to quickly integrate new elements into systems thus capitalizing
     on improvements made by other markets.
</li><li> Commonly much cheaper.
</li></ul>
<p>
Limitations
</p><ul>
<li> New technology.  Programmers need to learn parallel programming approaches.
</li><li> Standard sequential codes will not "just run".  
</li><li> Compilers and tools are often not mature.
</li><li> I/O is not as well understood yet.
</li></ul>

<p>
Parallel computing requires:
</p><p>
</p><ul>
<li> Multiple processors 
    <br>(The workers)
<p>
</p></li><li> Network
    <br> (Link between workers)
<p>
</p></li><li> Environment to create and manage parallel processing
     <ul>
     <p>
     </p><li> Operating System 
         <br>(Administrator of the system that knows how to
         handle multiple workers)
     <p>
     </p></li><li> Parallel Programming Paradigm
          <ul>
          <li> Message Passing
               <ul>
               <li> MPI
               </li><li> PVM
               </li></ul>
          </li><li> Data Parallel
               <ul>
               <li>Fortran 90 / High Performance Fortran
               </li></ul>
          </li><li> Others
               <ul>
               <li> OpenMP
               </li><li> shmem
               </li></ul>
          </li></ul>
     </li></ul>
<p>
</p></li><li> A parallel algorithm and a parallel program 
     <br> (The decomposition of the problem into pieces that
     multiple workers can perform)
</li></ul>



<a name="parallel programming"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Overview
</b></font></td><td bgcolor="navy" align="right">
<a href="#parallel%20computing">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Parallel Computing"></a>
<a href="#architecture%20taxonomy">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Architecture Taxonomy"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Parallel Programming
</b></font></td></tr></tbody></table>

<p>
</p><ul>
<li> Parallel programming involves:
     <ul>
     <p>
     </p><li> Decomposing an algorithm or data into parts 
     <p>
     </p></li><li> Distributing the parts as tasks which are worked on by multiple 
          processors simultaneously
     <p>
     </p></li><li> Coordinating work and communications of those processors
     </li></ul>
<p>
</p></li><li> Parallel programming considerations:
<p>
     </p><ul>
     <li> Type of parallel architecture being used
     </li><li> Type of processor communications used
     </li></ul>
</li></ul>

<p>


<a name="architecture taxonomy"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Architecture Taxonomy
</b></font></td><td bgcolor="navy" align="right">
<a href="#parallel%20programming">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Parallel Programming Overview"></a>
<a href="#sisd%20model">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to SISD Model"></a>
</td></tr></tbody></table>

</p><ul>
<p>
</p><li> All parallel computers use multiple processors
<p>
</p></li><li> There are several different methods used to classify computers 
<p>
</p></li><li> No single taxonomy fits all designs
<p>
</p></li><li> Not used as much today, but still see the categories
<p>
</p></li><li> Flynn's taxonomy uses the relationship of program 
     instructions to program data.  The four categories are:
     <ul>
     <p>
     </p><li><a href="#sisd%20model">
         SISD - Single Instruction, Single Data Stream  </a>
     <p>
     </p></li><li><a href="#simd%20model">
         SIMD - Single Instruction, Multiple Data Stream  </a>
     <p>
     </p></li><li> MISD - Multiple Instruction, Single Data Stream (no practical
          examples)
     <p>
     </p></li><li><a href="#mimd%20model">
         MIMD - Multiple Instruction, Multiple Data Stream  </a>
     </li></ul>
</li></ul>


<a name="sisd model"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Architecture Taxonomy
</b></font></td><td bgcolor="navy" align="right">
<a href="#architecture%20taxonomy">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Architecture Taxonomy"></a>
<a href="#simd%20model">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to SIMD Model"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
SISD Model:     Single Instruction, Single Data Stream
</b></font></td></tr></tbody></table>

<ul>
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/sm_sisd.gif" target="W2" onclick="blur()">
<img src="IntroPara_files/sm_sisd.gif" width="266" height="316"></a> 
<p>
</p><li> Not a parallel computer
<p>
</p></li><li> Conventional serial, scalar von Neumann computer 
<p>
</p></li><li> One instruction stream  
<p>
</p></li><li> A single instruction is issued each clock cycle 
<p>
</p></li><li> Each instruction operates on a single (scalar) data element
<p>
</p></li><li> Limited by the number of instructions that can be issued in a given unit 
    of time
<p>
</p></li><li> Performance frequently measured in MIPS (million of instructions per
     second) or clock frequency MHz (Megahertz)
<p>
</p></li><li> Most non-supercomputers
<p>
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/intro_analogy_sisd.html#sisd" target="W2" onclick="blur()">
<img src="IntroPara_files/modelt.gif" width="140" height="88">Automobile analogy</a>
</p></li></ul>
<p>


<a name="simd model"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Architecture Taxonomy
</b></font></td><td bgcolor="navy" align="right">
<a href="#sisd%20model">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to SISD Model"></a>
<a href="#mimd%20model">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to MIMD Model"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
SIMD Model:      Single Instruction, Multiple Data Stream
</b></font></td></tr></tbody></table>

</p><ul>
<p>
</p><li> Also von Neumann architectures but more powerful instructions
<p>
</p></li><li> Each instruction may operate on more than one data element
<p>
</p></li><li> Usually intermediate host executes program logic and broadcasts 
     instructions to other processors
<p>
</p></li><li> Synchronous (lockstep)
<p>
</p></li><li> Rating how fast these machines can issue instructions
     is not a good measure of their performance
<p>
</p></li><li> Performance is measured in MFLOPS (millions of floating point 
     operations per second)
<p>
</p></li><li> Two major types: 
     <ul>
     <p>
     </p><li> Vector SIMD
     <p>
     </p></li><li> Parallel SIMD
     </li></ul>
<p>
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/intro_analogy_simd.html#simd" target="W2" onclick="blur()">
<img src="IntroPara_files/wagon.gif" width="176" height="72">Automobile analogy</a> 
</p></li></ul>



<p> </p><hr> <p>
<font size="+2" face="helvetica"><b>
Vector SIMD 
</b></font>
</p><p>

</p><ul>
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/vector.gif" target="W2" onclick="blur()">
<img src="IntroPara_files/vector.gif" width="503" height="389"></a>
<p>
</p><li>Single instruction results in multiple operands being updated
<p>
</p></li><li> Scalar processing operates on single data elements.  Vector processing 
     operates on whole vectors (groups) of data at a time.
<p>
</p></li><li> Examples:
     <dl><dd> Cray 1
     </dd><dd> NEC SX-2
     </dd><dd> Fujitsu VP 
     </dd><dd> Hitachi S820
     </dd></dl>
<p>
     Single processor of:
     </p><dl><dd> Cray C 90
     </dd><dd> Cray2
     </dd><dd> NEC SX-3
     </dd><dd> Fujitsu VP 2000
     </dd><dd> Convex C-2
     </dd></dl>
</li></ul>
<p>


</p><p> </p><hr> <p>
<font size="+2" face="helvetica"><b>
Parallel SIMD 
</b></font>
</p><p>

</p><ul>
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/simd.gif" target="W2" onclick="blur()">
<img src="IntroPara_files/simd.gif" width="559" height="432"></a>
<p>
</p><li> Processor arrays -  single instruction is issued and all processors 
     execute the same instruction, operating on different sets of data. 
<p>
</p></li><li> Processors run in a synchronous, lockstep fashion
<p>
</p></li><li> Advantages
     <ul>
     <p>
     </p><li> DO loops conducive to SIMD parallelism
     <pre>     
	do 100 i= 1, 100
          c(i) = a(i) + b(i)
   100  continue
     </pre>

     <p>
     </p></li><li> Synchronization not a problem - all processors operate in lock-step
     </li></ul>
<p>
</p></li><li> Disadvantages
     <ul>
     <p>
     </p><li> Decisions within DO loops can result in poor execution by requiring
          all processes to perform the operation controlled by decision
          whether results are used or not
     </li></ul>
<p>
</p></li><li> Examples: 
     <dl><dd> Connection Machine CM-2
     </dd><dd> Maspar MP-1, MP-2
     </dd></dl>
</li></ul>


<a name="mimd model"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Architecture Taxonomy
</b></font></td><td bgcolor="navy" align="right">
<a href="#simd%20model">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to SIMD Model"></a>
<a href="#memory%20architectures">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Memory Architectures"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
MIMD Model:      Multiple Instructions, Multiple Data 
</b></font></td></tr></tbody></table>

<ul>
<p>
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/mimd.gif" target="W2" onclick="blur()">
<img src="IntroPara_files/mimd.gif" width="559" height="432"></a>
</p><p>
</p><li> Parallelism achieved by connecting multiple processors together
<p>
</p></li><li> Includes all forms of multiprocessor configurations 
<p>
</p></li><li> Each processor executes its own instruction stream independent of other
     processors on unique data stream  
<p>
</p></li><li> Advantages
     <ul>
     <p>
     </p><li> Processors can execute multiple job streams simultaneously
     <p>
     </p></li><li> Each processor can perform any operation regardless of what other
          processors are doing
     </li></ul>
<p>
</p></li><li> Disadvantages
     <ul>
     <p>
     </p><li> Load balancing overhead - synchronization needed to coordinate
          processors at end of parallel structure in a single application
     <p>
     </p></li><li> Can be difficult to program
     </li></ul>
<p>
</p></li><li> Examples
<p>
     MIMD Accomplished via Parallel SISD machines: 
     </p><dl><dd> Sequent
     </dd><dd> nCUBE
     </dd><dd> Intel iPSC/2
     </dd><dd> IBM RS6000 cluster
     </dd></dl>
<p>
     MIMD Accomplished via Parallel SIMD machines: 
     </p><dl><dd> Cray C 90
     </dd><dd> Cray 2
     </dd><dd> NEC SX-3
     </dd><dd> Fujitsu VP 2000
     </dd><dd> Convex C-2
     </dd><dd> Intel Paragon
     </dd><dd> CM 5
     </dd><dd> KSR-1 
     </dd><dd> IBM SP1
     </dd><dd> IBM SP2
     </dd></dl>
<p>
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/intro_analogy_mimd.html#mimd" target="W2" onclick="blur()">
<img src="IntroPara_files/porche.gif" width="128" height="80">Automobile analogy</a> 
</p></li></ul> 


<a name="memory architectures"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Memory Architectures 
</b></font></td><td bgcolor="navy" align="right">
<a href="#mimd%20model">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to MIMD Model"></a>
<a href="#shared%20memory">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Shared Memory"></a>
</td></tr></tbody></table>

<p>
</p><ul>
<li> The way processors communicate is dependent upon memory architecture,
     which, in turn, will affect how you write your parallel program 
<p>
</p></li><li> The primary memory architectures are:
     <ul>
     <p>
     </p><li> Shared Memory
     <p>
     </p></li><li> Distributed Memory
     </li></ul>
</li></ul>


<a name="shared memory"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Memory Architectures
</b></font></td><td bgcolor="navy" align="right">
<a href="#memory%20architectures">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Memory Architectures"></a>
<a href="#distributed%20memory">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Distributed Memory"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Shared Memory 
</b></font></td></tr></tbody></table>

<ul>
<p>
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/sharemem.gif" target="W2" onclick="blur()">
<img src="IntroPara_files/sharemem.gif" width="525" height="369"></a> 
</p><p>
</p><li> Multiple processors operate independently but share the same memory
     resources
<p>
</p></li><li> Only one processor can access the shared memory location at a time
<p>
</p></li><li> Synchronization achieved by controlling tasks' reading from and
     writing to the shared memory
<p>
</p></li><li> Advantages
     <ul>
     <p>
     </p><li> Easy for user to use efficiently 
     <p>
     </p></li><li> Data sharing among tasks is fast (speed of memory access)
     </li></ul>
<p>
</p></li><li> Disadvantages
     <ul>
     <p>
     </p><li> Memory is bandwidth limited. Increase of processors without 
          increase of bandwidth can cause severe bottlenecks
     <p>
     </p></li><li> User is responsible for specifying synchronization, e.g., locks
     </li></ul>
<p>
</p></li><li> Examples:
     <ul>
     <li> Cray Y-MP
     </li><li> Convex C-2
     </li><li> Cray C-90
     </li></ul>
</li></ul>


<a name="distributed memory"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Memory Architectures
</b></font></td><td bgcolor="navy" align="right">
<a href="#shared%20memory">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Shared Memory"></a>
<a href="#memory-processor%20arrangements">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Memory-Processor Arrangements"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Distributed Memory  
</b></font></td></tr></tbody></table>

<ul>
<p>
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/distmem.gif" target="W2" onclick="blur()">
<img src="IntroPara_files/distmem.gif" width="700" height="400"></a>
</p><p>
</p><li> Multiple processors operate independently but
     each has its own private memory 
<p>
</p></li><li> Data is shared across a communications network using message passing
<p>
</p></li><li> User responsible for synchronization using message passing 
<p>
</p></li><li> Advantages
     <ul>
     <p>
     </p><li> Memory scalable to number of processors.  Increase number of
          processors, size of memory and bandwidth increases.
     <p>
     </p></li><li> Each processor can rapidly access its own memory without interference
     </li></ul>
<p>
</p></li><li>
    Disadvantages
     <ul>
     <p>
     </p><li> Difficult to map existing data structures to this memory organization
     <p>
     </p></li><li> User responsible for sending and receiving data among processors
     <p>
     </p></li><li> To minimize overhead and latency, data should be blocked up in
          large chunks and shipped before receiving node needs it
     </li></ul>
<p>
</p></li><li> Examples:
     <ul>
     <li> nCUBE Hypercube
     </li><li> Intel Hypercube
     </li><li> TMC CM-5
     </li><li> IBM SP1, SP2 
     </li><li> Intel Paragon
     </li></ul>
</li></ul>


<a name="memory-processor arrangements"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Memory Architectures
</b></font></td><td bgcolor="navy" align="right">
<a href="#distributed%20memory">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Distributed Memory"></a>
<a href="#communications%20network">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Communications Network on the IBM SP"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Memory / Processor Arrangements
</b></font></td></tr></tbody></table>

<ul>
<p>
</p><li> Distributed Memory
     <ul>
     <p>
     </p><li> MPP - Massively Parallel Processor
     </li></ul>
<p>
</p></li><li> Shared Memory
     <ul>
     <p>
     </p><li> SMP - Symmetric Multiprocessor
         <ul>
         <p>
         </p><li> Identical processors
         <p>
         </p></li><li> Equal access to memory
         <p>
         </p></li><li> Sometimes called UMA - Uniform Memory Access
         <p>
         </p></li><li> or CC-UMA - Cache Coherent UMA
         <p>
         </p></li><li> Cache coherent means if one processor updates a location in
              shared memory, all the other processors know about the update
         </li></ul>
     <p>
     </p></li><li> NUMA - Non-Uniform Memory Access
         <ul>
         <p>
         </p><li> Sometimes called CC-NUMA - Cache Coherent NUMA
         <p>
         </p></li><li> Often made by linking two or more SMPs
         <p>
         </p></li><li> One SMP can directly access memory of another SMP
         <p>
         </p></li><li> Not all processors have equal access time to all memories
         <p>
         </p></li><li> Memory access across link is slower
         </li></ul>
     </li></ul>
<p>
</p></li><li> Combinations
     <ul>
     <p>
     </p><li> Multiple SMPs connected by a network
          <ul>
          <p>
          </p><li> Processors within an SMP communicate via memory
          <p>
          </p></li><li> Requires message passing between SMPs
          <p>
          </p></li><li> One SMP can't directly access the memory of another SMP
          </li></ul>
     <p>
     </p></li><li> Multiple distributed memory processors connected to a larger shared
          memory
          <ul>
          <p>
          </p><li> Small fast memory can be used for supplying data to processors
               and large slower memory can be used for a backfill to the
               smaller memories
          <p>
          </p></li><li> Similar to register &lt;= cache memory &lt;= main memory hierarchy
          <p>
          </p></li><li> Transfer from local memory to shared memory would be transparent
               to the user
          <p>
          </p></li><li> Probable design of the future with several processors and
               their local memory surrounding a larger shared memory on a
               single board 
          <p>
          </p></li></ul>
     </li></ul>
<p>
</p></li><li> Comparison
<p>
     </p><ul>
     <table>
     <caption><b>Mulitprocessor Architectures</b><hr></caption>
     <tbody><tr><th> </th><th>CC-UMA </th><th>CC-NUMA </th><th>MPP </th></tr><tr>
     <td><hr> </td><td><hr> </td><td><hr> </td><td><hr> </td></tr><tr>

     <td><b>Architecture</b>
     </td><td>mostly RISC </td><td>mostly RISC </td><td>RISC </td></tr><tr>
     <td> </td><td> </td><td> </td><td>rich instructions </td></tr><tr>
     <td><hr> </td><td><hr> </td><td><hr> </td><td><hr> </td></tr><tr>

     <td><b>Examples</b>
     </td><td>SMPs </td><td>SGI Origin </td><td>Cray T3E </td></tr><tr>
     <td> </td><td>Sun VExx </td><td>Sequent </td><td>Maspar </td></tr><tr>
     <td> </td><td>DEC </td><td>HP Exemplar </td><td>IBM SP2 </td></tr><tr>
     <td> </td><td>SGI Challenge </td><td>DEC </td><td> </td></tr><tr>
     <td><hr> </td><td><hr> </td><td><hr> </td><td><hr> </td></tr><tr>

     <td><b>Communications</b>
     </td><td>MPI </td><td>MPI </td><td>MPI </td></tr><tr>
     <td> </td><td>shmem </td><td>shmem </td><td>  </td></tr><tr>
     <td><hr> </td><td><hr> </td><td><hr> </td><td><hr> </td></tr><tr>

     <td><b>Scalabiliity</b>
     </td><td>to 10s of </td><td>to 100s of </td><td>to 1000s of </td></tr><tr>
     <td> </td><td>processors </td><td>processors </td><td>processors </td></tr><tr>
     <td><hr> </td><td><hr> </td><td><hr> </td><td><hr> </td></tr><tr>

     <td><b>Draw Backs</b>
     </td><td>limited memory </td><td>new architecture </td><td>sys admin </td></tr><tr>
     <td> </td><td>bandwidth </td><td> </td><td>programming </td></tr><tr>
     <td><hr> </td><td><hr> </td><td><hr> </td><td><hr> </td></tr><tr>

     <td><b>Software</b>
     </td><td>many 1000s ISVs</td><td>many 1000s ISVs</td><td>10s ISVs </td></tr><tr>
     <td><b>Availablilty</b>
     </td><td> </td><td>point-to-point </td><td>hard to develop </td></tr><tr>
     <td> </td><td> </td><td>communication </td><td>and maintain </td></tr><tr>
     <td> </td><td> </td><td> </td><td>"home grown" </td></tr><tr>
     <td><hr> </td><td><hr> </td><td><hr> </td><td><hr> </td></tr><tr>

     </tr></tbody></table>
     </ul>
</li></ul>



<a name="communications network"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Processor Communications
</b></font></td><td bgcolor="navy" align="right">
<a href="#memory-processor%20arrangements">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Memory-Processor Arrangements"></a>
<a href="#paradigms">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Parallel Programming Paradigms"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Communications Network on the IBM SP
</b></font></td></tr></tbody></table>

<ul>
<li> In order to coordinate tasks of multiple nodes working on the same
     problem, some form of inter-processor communications is required to:
     <ul>
     <p>
     </p><li>Convey information and data between processors 
     </li><li>Synchronize node activities
     </li></ul>
<p>
</p></li></ul>

<ul>
<p>
</p><li> Distributed memory
<p>
</p></li><li> SP Node Connectivity      
     <ul>
     <p>
     </p><li> Nodes are connected to each other by a native high performance switch
     <p>
     </p></li><li> Nodes are connected to the network (and, hence, to each other) 
          via the ethernet 
     </li></ul>
<p>
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/connect.gif" target="W2" onclick="blur()">
<img src="IntroPara_files/connect.gif" width="621" height="480"></a>
</p><p>
</p></li><li> Type of Communication
     <ul>
     <p>
     </p><li> ip - Internet Protocol (TCP/IP)
          <ul>
          <li> runs over ethernet or
          </li><li> runs over the switch (depending on environment setup)
          </li></ul>
     <p>
     </p></li><li> us - User Space 
          <ul>
          <li> runs over the switch
          </li></ul>
     </li></ul>
</li></ul>




<a name="paradigms"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Parallel Programming Paradigms
</b></font></td><td bgcolor="navy" align="right">
<a href="#communications%20network">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Communications Network on the IBM SP"></a>
<a href="#MP%20Definition">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Message Passing"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Various Methods 
</b></font></td></tr></tbody></table>

<p>
There are many methods of programming parallel computers.  Two of the most
common are message passing and data parallel.
</p><ul>
<li> Message Passing - the user makes calls to libraries to explicitly 
     share information between processors.
<p> 
</p></li><li> Data Parallel - data partitioning determines parallelism 
<p> 
</p></li><li> Shared Memory - multiple processes sharing common memory space
<p> 
</p></li><li> Remote Memory Operation - set of processes in which a process can 
     access the memory of another process without its participation
<p> 
</p></li><li> Threads - a single process having multiple (concurrent)
     execution paths 
<p> 
</p></li><li> Combined Models -  composed of two or more of the above.
</li></ul>

<p> 
Note: these models are machine/architecture
independent, any of the models can be implemented on
any hardware given appropriate operating system
support.<br>

An effective implementation is one which closely matches its
target hardware and provides the user ease in programming.



<a name="MP Definition"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Parallel Programming Paradigms
</b></font></td><td bgcolor="navy" align="right">
<a href="#paradigms">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Various Methods"></a>
<a href="#DP%20Definition">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Data Parallel"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Message Passing
</b></font></td></tr></tbody></table>

</p><p>
The message passing model is defined as:
</p><ul>
<li> set of processes using only local memory 
<p>
</p></li><li> processes communicate by sending and receiving
     messages 
<p>
</p></li><li> data transfer requires
     cooperative operations to be performed by each
     process (a send operation must have a matching
     receive) 
</li></ul>

<p>
Programming with message passing is done by linking with and
making calls to libraries which manage the data exchange 
between processors.  Message passing libraries are available for most
modern programming languages.



<a name="DP Definition"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Parallel Programming Paradigms
</b></font></td><td bgcolor="navy" align="right">
<a href="#MP%20Definition">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Message Passing"></a>
<a href="#Implementations">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Implementations"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Data Parallel
</b></font></td></tr></tbody></table>

</p><p> 
The data parallel model is defined as:
</p><ul>
<li> Each process works on a different part of the same data structure
<p> 
</p></li><li> Commonly a Single Program Multiple Data (SPMD) approach
<p> 
</p></li><li> Data is distributed across processors
<p> 
</p></li><li> All message passing is done invisibly to the programmer
<p> 
</p></li><li> Commonly built "on top of" one of the common message passing libraries 
</li></ul>
<p>
Programming with data parallel model is accomplished by
writing a program with data parallel constructs and compiling it with
a data parallel compiler.
</p><p>
The compiler converts the program into standard
code and calls to a message passing library to distribute the data to all
the processes.  



<a name="Implementations"> <br><br>   </a>
<a name="MPI Overview">   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Parallel Programming Paradigms
</b></font></td><td bgcolor="navy" align="right">
<a href="#DP%20Definition">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Data Parallel"></a>
<a href="#HPF%20Overview">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to F90 / High Performance Fortran (HPF)"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Implementations</b></font><p>
<font size="+2" face="helvetica"><b>Message Passing: Message Passing Interface (MPI)
</b></font></p></td></tr></tbody></table>

</p><p> 
</p><ul>
<li> Message Passing Interface often called MPI.
<p>
</p></li><li> A standard portable message-passing library definition developed in
     1993 by a group of parallel computer vendors, software writers, and
     application scientists.
<p>
</p></li><li> Available to both Fortran and C programs.
<p>
</p></li><li> Available on a wide variety of parallel machines.
<p>
</p></li><li> Target platform is a distributed memory system such as the SP.
<p>
</p></li><li> All inter-task communication is by message passing.
<p>
</p></li><li> All parallelism is explicit: the programmer is responsible for
     parallelism the program and implementing the MPI constructs.
<p>
</p></li><li> Programming model is SPMD (Single Program Multiple Data)
</li></ul>


<a name="HPF Overview"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Parallel Programming Paradigms
</b></font></td><td bgcolor="navy" align="right">
<a href="#MPI%20Overview">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Message Passing"></a>
<a href="#steps%20for%20creating%20parallel%20program">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Steps for Creating a Parallel Program"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Implementations</b></font><p>
<font size="+2" face="helvetica"><b>F90 / High Performance Fortran (HPF)
</b></font></p></td></tr></tbody></table>

<ul>
<p> 
</p><li> Fortran 90 (F90) - (ISO / ANSI standard extensions to Fortran 77).
<p> 
</p></li><li> High Performance Fortran (HPF) - extensions to F90 to support data parallel
     programming.
<p> 
</p></li><li> Compiler directives allow programmer specification of data distribution
     and alignment.
<p> 
</p></li><li> New compiler constructs and intrinsics allow the programmer to do 
     computations and manipulations on data with different distributions.
</li></ul>
<p> 


<a name="steps for creating parallel program"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Steps for Creating a Parallel Program
</b></font></td><td bgcolor="navy" align="right">
<a href="#HPF%20Overview">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to F90 / High Performance Fortran (HPF)"></a>
<a href="#decomposing%20the%20program">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Decomposing the Program"></a>
</td></tr></tbody></table>

</p><ol>
<p>
</p><li> If you are starting with an existing serial program, debug the serial code 
     completely
<p>
</p></li><li> Identify the parts of the program that can be executed concurrently:  
     <ul>
     <p>
     </p><li> Requires a thorough understanding of the algorithm
     <p>
     </p></li><li> Exploit any inherent parallelism which may exist.
     <p>
     </p></li><li> May require restructuring of the program and/or algorithm. 
          May require an entirely new algorithm.
     </li></ul>
<p>
</p></li><li> Decompose the program:
     <ul>
     <p>
     </p><li> Functional Parallelism
     </li><li> Data Parallelism 
     </li><li> Combination of both
     </li></ul>
<p>
</p></li><li>Code development
     <ul>
     <p>
     </p><li> Code may be influenced/determined by machine architecture
     <p>
     </p></li><li> Choose a programming paradigm 
     <p>
     </p></li><li> Determine communication     
     <p>
     </p></li><li> Add code to accomplish task control and communications
     </li></ul>
<p>
</p></li><li> Compile, Test, Debug
<p>
</p></li><li> Optimization
     <ul>
     <p>
     </p><li>Measure Performance
     </li><li>Locate Problem Areas
     </li><li>Improve them
     </li></ul>
</li></ol>



<a name="decomposing the program"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Steps for Creating a Parallel Program
</b></font></td><td bgcolor="navy" align="right">
<a href="#steps%20for%20creating%20parallel%20program">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Steps for Creating a Parallel Program"></a>
<a href="#Communication">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Communication"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Decomposing the Program
</b></font></td></tr></tbody></table>

<p>
There are three methods for decomposing a problem into smaller tasks to be 
performed in parallel: Functional Decomposition, Domain Decomposition,
or a combination of both
</p><p>
</p><ul>
<p>
</p><li><a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/function.decomp.gif" target="W2" onclick="blur()"> 
     Functional Decomposition (Functional Parallelism)</a>
     <ul>
     <p>
     </p><li> Decomposing the problem into different tasks which can be
          distributed to multiple processors for simultaneous execution
     <p>
     </p></li><li> Good to use when there is not static structure or fixed determination
          of number of calculations to be performed
     </li></ul>      
<p>
</p></li><li><a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/domain.decomp.gif" target="W2" onclick="blur()"> 
    Domain Decomposition (Data Parallelism) </a>
     <ul>
     <p>
     </p><li> Partitioning the problem's data domain and distributing portions to
          multiple processors for simultaneous execution
     <p>
     </p></li><li> Good to use for problems where:
          <ul>
          <p>
          </p><li> data is static (factoring and solving large matrix or finite 
               difference calculations) 
          <p>
          </p></li><li> dynamic data structure tied to single entity where entity can be 
               subsetted (large multi-body problems)
          <p>
          </p></li><li> domain is fixed but computation within various regions of the
               domain is dynamic (fluid vortices models)
          </li></ul>
     </li></ul>
<p>
</p></li><li>
There are many ways to decompose data into partitions to be distributed:

     <ul>
     <p>
     </p><li><a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/1d.gif" target="W2" onclick="blur()">One Dimensional 
         Data Distribution </a>
     <p>
          </p><ul>
          <li><a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/1dblock.gif" target="W2" onclick="blur()">Block 
              Distribution</a>
          </li><li><a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/1dcyclic.gif" target="W2" onclick="blur()">Cyclic 
              Distribution</a>
          </li></ul>
     <p>
     </p></li><li><a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/2d.gif" target="W2" onclick="blur()">Two Dimensional Data
          Distribution </a>
          <p>
          </p><ul>
          <li><a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/2dbb.gif" target="W2" onclick="blur()">Block Block 
              Distribution </a>
          </li><li><a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/2dbc.gif" target="W2" onclick="blur()">Block Cyclic 
              Distribution </a>
          </li><li><a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/2dcb.gif" target="W2" onclick="blur()">Cyclic Block 
              Distribution </a>
          </li></ul>
     </li></ul>    
</li></ul>



<a name="Communication"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Steps for Creating a Parallel Program
</b></font></td><td bgcolor="navy" align="right">
<a href="#decomposing%20the%20program">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Decomposing the Program"></a>
<a href="#design%20and%20performance%20considerations">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Design and Performance Considerations"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Communication
</b></font></td></tr></tbody></table>

<p>
Understanding the interprocessor communications of your program is essential.  
</p><ul>
<p>
</p><li> Message Passing communication is programed explicitly.  The programmer
     must understand and code the communication.
<p>
</p></li><li> Data Parallel compilers and run-time systems do all 
     communications behind the scenes.  The programmer need not understand
     the underlying communications.  On the other hand to get good
     performance from your code you should write your algorithm with the
     best communication possible.
<p>
</p></li></ul>
The types of communications for message passing and data parallel are exactly
the same.  In fact most data parallel compilers simply use one of the 
standard message passing libraries to achieve data movement.<br><br>

Communications on distributed memory computers:
<ul>
<p>
</p><li> Point to Point
</li><li> One to All Broadcast
</li><li> All to All Broadcast
</li><li> One to All Personalized
</li><li> All to All Personalized
</li><li> Shifts
</li><li> Collective Computation
</li></ul>

<a name="Comm Point to Point"></a>   <p></p><hr><p> 
<font size="+2" face="helvetica"><b>
Point to Point
</b></font>

</p><p>
The most basic method of communication between two processors is the 
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/basic_msg.gif" target="W2" onclick="blur()">
point to point message</a>. The originating processor "sends" the message
to the destination processor.  The destination processor then "receives"
the message.
</p><p>
The message commonly includes the information, the length of the message, the
destination address and possibly a tag. 
</p><p>
Typical message passing libraries subdivide the basic sends
and receives into two types: 
</p><ul>
<li> blocking - processing waits until message is transmitted
</li><li> nonblocking - processing continues even if message hasn't been
     transmitted yet
</li></ul>


<a name="Comm One to All B"></a>   <p></p><hr><p> 
<font size="+2" face="helvetica"><b>
One to All Broadcast
</b></font>

</p><p>
A node may have information which all the others require.  
A <a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/broadcast.gif" target="W2" onclick="blur()">broadcast</a>
is a message sent to many other nodes.
</p><p>
A One to All broadcast occurs when one processor sends the same information 
to many other nodes.


<a name="Comm All to All B"></a>   </p><p></p><hr><p> 
<font size="+2" face="helvetica"><b>
All to All Broadcast
</b></font>

</p><p>
With an All to All 
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/all2all.gif" target="W2" onclick="blur()">broadcast</a> each 
processor sends its unique information to all the other processors.



<a name="One to All P"></a>   </p><p></p><hr><p> 
<font size="+2" face="helvetica"><b>
One to All Personalized
</b></font>

</p><p>
Personalized communication send a unique message to each processor.
</p><p>
In One to All personalized communication one processor sends a unique
message to every other processor.


<a name="Comm All to All P"></a>   </p><p></p><hr><p> 
<font size="+2" face="helvetica"><b>
All to All Personalized
</b></font>

</p><p>
In All to All Personalized communication each processor sends a unique message
to all other processors.


<a name="Comm Shifts"></a>   </p><p></p><hr><p> 
<font size="+2" face="helvetica"><b>
Shifts
</b></font>

</p><p>
Shifts are permutations of information.  Information is exchanged in one 
logical direction or the other.  Each processor exchanges the same amount of 
information with its neighbor processor.
</p><p>
There are two types of shifts:<br>
</p><ul>
<li> Circular - Each processor exchanges information with its logical neighbor.
     When there is no longer a neighbor due to an edge of data the shift
     "wraps around" and takes the information from the opposite edge.<br>
</li><li> End Off Shift - When an edge occurs, the processor is padded with zero or 
     a user defined value.
</li></ul>



<a name="Comm Collective"></a>   <p></p><hr><p> 
<font size="+2" face="helvetica"><b>
Collective Computation
</b></font>

</p><p>
In collective computation (reductions), one member of the group
collects data from the other members.  Commonly a mathematical
operation like a min, max, add, multiple etc. is performed.
   


<a name="design and performance considerations"> <br><br>   </a>
<a name="amdahl">   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Design and Performance Considerations
</b></font></td><td bgcolor="navy" align="right">
<a href="#Communication">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Communication"></a>
<a href="#load%20balancing">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Load Balancing"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Amdahl's Law
</b></font></td></tr></tbody></table>

</p><ul>
<p>
</p><li> <a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/amdahl1.gif" target="W2" onclick="blur()">Amdahl's Law</a> 
    states that potential program
    speedup is defined by the fraction of code (P) which can be parallelized:
<pre><b>
                     1
    speedup   =   -------- 
                   1  - P
</b></pre>
<p>
</p></li><li> If none of the code can be parallelized, f  = 0 and the speedup = 1 (no
     speedup).  If all of the code is parallelized, f = 1 and the speedup is
     infinite (in theory).
     <p>
     If 50% of the code can be parallelized, maximum speedup = 2, meaning
     the code will run twice as fast.
</p><p>
</p></li><li> Introducing the number of processors performing the parallel fraction of 
     work, the relationship can be modeled by:
<pre><b>
                       1  
    speedup   =   ------------ 
                    P   +  S
                   ---
                    N
</b></pre>
    where P = parallel fraction, N = number of processors  and S = serial 
    fraction.
<p>
</p></li><li> It soon becomes obvious that there are limits to the scalability of 
     parallelism.  For example, at P = .50, .90 and .99  (50%, 90% and 99% of 
     the code is parallelizable): 
<pre><b>
                          speedup
                --------------------------------
       N        P = .50      P = .90     P = .99
     -----      -------      -------     -------
        10         1.82         5.26        9.17
       100         1.98         9.17       50.25     
      1000         1.99         9.91       90.99
     10000         1.99         9.91       99.02

</b></pre>    
<p>
</p></li><li> However, certain problems demonstrate increased performance by increasing
     the problem size.  For example:
<pre><b>
    2D Grid Calculations     85 seconds   85%
    Serial fraction          15 seconds   15%
</b></pre>
     We can increase the problem size by halving both the grid points and
     the time step, which is directly proportional to the grid spacing.
     This results in four times the number of grid points (factor of two in 
     each direction) and twice the number of time steps. The timings then
     look like:
<pre><b>
    2D Grid Calculations     680 seconds   97.84%
    Serial fraction           15 seconds    2.16%
</b></pre>
<p>
</p></li><li> Problems which increase the percentage of parallel time with their size
     are more "scalable" than problems with a fixed percentage of parallel
     time.
</li></ul>



<a name="load balancing"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Design and Performance Considerations
</b></font></td><td bgcolor="navy" align="right">
<a href="#amdahl">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Amdahls Law"></a>
<a href="#granularity">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Granularity"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Load Balancing 
</b></font></td></tr></tbody></table>

<p>
</p><ul>
<li> Load balancing refers to the distribution of tasks in such a way as to
     insure the most time efficient parallel execution
<p>
</p></li><li> If tasks are not distributed in a balanced way, you may end up waiting
     for one task to complete while other tasks are idle 
<p>
</p></li><li> Performance can be increased if work can be more evenly distributed
     <p>
     For example, if there are many tasks of varying sizes, it may be more 
     efficient to maintain a task pool and distribute to processors as 
     each finishes 
</p><p>
</p></li><li> Consider a heterogeneous environment where there are machines of widely 
     varying power and user load versus a homogeneous environment with
     identical processors running one job per processor
</li></ul>



<a name="granularity"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Design and Performance Considerations
</b></font></td><td bgcolor="navy" align="right">
<a href="#load%20balancing">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Load Balancing"></a>
<a href="#data%20dependency">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Data Dependency"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Granularity 
</b></font></td></tr></tbody></table>

<p>
</p><ul>
<li> In order to coordinate between different processors working on the same 
     problem, some form of communication between them is required
<p>
</p></li><li> The ratio between computation and communication is known as granularity.
<p>
</p></li><li><a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/finegran.gif" target="W2" onclick="blur()">Fine-grain 
    parallelism</a> 
     <ul> 
     <p>
     </p><li> All tasks execute a small number of instructions between
          communication cycles
     <p>
     </p></li><li> Low computation to communication ratio  
     <p>
     </p></li><li> Facilitates load balancing 
     <p>
     </p></li><li> Implies high communication overhead and less opportunity for
          performance enhancement
     <p>
     </p></li><li> If granularity is too fine it is possible that the overhead
          required for communications and synchronization between tasks
          takes longer than the computation. 
     </li></ul>
<p>
</p></li><li><a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/coarsegran.gif" target="W2" onclick="blur()">Coarse-grain 
    parallelism </a> 
     <ul>
     <p>
     </p><li> Typified by long computations consisting of large numbers of
          instructions between communication synchronization points
     <p>
     </p></li><li> High computation to communication ratio
     <p>
     </p></li><li> Implies more opportunity for performance increase
     <p>
     </p></li><li> Harder to load balance efficiently
     </li></ul>
<p>
</p></li><li> The most efficient granularity is dependent on the algorithm and the 
     hardware environment in which it runs
<p>
</p></li><li> In most cases overhead associated with communications and 
     synchronization is high relative to execution speed
     so it is advantageous to have coarse granularity.
</li></ul>


<a name="data dependency"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Design and Performance Considerations
</b></font></td><td bgcolor="navy" align="right">
<a href="#granularity">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Granularity"></a>
<a href="#deadlock">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Deadlock"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Data Dependency 
</b></font></td></tr></tbody></table>

<p>
</p><ul>
<li> A data dependency exists when there is multiple use of the same storage
     location
<p>
</p></li><li> Importance of dependencies: frequently inhibit parallel execution
<p>
</p></li><li> Example 1: 
<pre><b>
        DO 500 J = MYSTART,MYEND
        A(J) = A(J-1) * 2.0
    500 CONTINUE
</b></pre>
     <ul>
     <li> This code has a data dependency.
     <p>
     </p></li><li> Must have computed value for A(J-1) before we can calculate A(J).
     <p>
     </p></li><li> If Task 2 has A(J) and Task 1 has A(J-1), the value of A(J) is
          dependent on: 
          <ul>
          <p>
          </p><li> Distributed memory
               <p>
               Task 2 obtaining the value of A(J-1) from Task 1
               </p><p>
          </p></li><li> Shared memory
               <p>
               Whether Task 2 reads A(J-1) before or after Task 1 updates it
          </p></li></ul>
     </li></ul>
<p>
</p></li><li> Example 2: 
<pre><b>
    task 1        task 2
    ------        ------

    X = 2         X = 4
      .             .
      .             .

    Y = X**2      Y = X**3
</b></pre>
     <ul>
     <p>
          The value of Y is dependent on: 
          </p><p>
     </p><li> Distributed memory 
          <p>
          If and/or when the value of X is communicated between the tasks.
          </p><p>
     </p></li><li> Shared memory
          <p>
          Which task last stores the value of X.  
     </p></li></ul>
<p>
</p></li><li> How to handle data dependencies?  
     <ul>
     <p>
     </p><li> Distributed memory
          <p>
          Communicate required data at synchronization points.
          </p><p>
     </p></li><li> Shared memory
          <p>
          Synchronize read/write operations between tasks.  
     </p></li></ul>
</li></ul>

<a name="deadlock"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Design and Performance Considerations
</b></font></td><td bgcolor="navy" align="right">
<a href="#data%20dependency">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Data Dependency"></a>
<a href="#bandwidth">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Communication Patterns and Bandwidth"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Deadlock 
</b></font></td></tr></tbody></table>

<p>
</p><ul>
<li> Deadlock describes a condition where two or more processes are waiting
     for an event or communication from one of the other processes.
<p>
</p></li><li> The simplest example is demonstrated by two processes which are both
     programmed to read/receive from the other before writing/sending.
<p>
</p></li><li> Example
<pre><b>
         TASK1                   TASK2
    ------------------     ------------------

    X = 4                  Y = 8

    SOURCE = TASK2         SOURCE = TASK1
    RECEIVE (SOURCE,Y)     RECEIVE (SOURCE,X)

    DEST = TASK2           DEST = TASK1
    SEND (DEST, X)         SEND (DEST, Y)

    Z = X + Y              Z = X + Y

</b></pre>
<p>
</p></li><li> One solution is to change the order of the <b>SEND</b> and
     <b>RECEIVE</b> in one of the tasks.
<pre><b>
         TASK1                   TASK2
    ------------------     ------------------

    X = 4                  Y = 8

    SOURCE = TASK2         DEST = TASK1
    RECEIVE (SOURCE,Y)     SEND (DEST, Y)

    DEST = TASK2           SOURCE = TASK1
    SEND (DEST, X)         RECEIVE (SOURCE,X)

    Z = X + Y              Z = X + Y

</b></pre>
</li><li> Another solution is to use NON-BLOCKING message passing.
</li></ul>





<a name="bandwidth"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Design and Performance Considerations
</b></font></td><td bgcolor="navy" align="right">
<a href="#deadlock">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Deadlock"></a>
<a href="#io%20patterns">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to I/O Patterns"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Communication Patterns and Bandwidth
</b></font></td></tr></tbody></table>

<p>
</p><ul>
<li> For some problems, increasing the number of processors will:
     <ul>
     <p>
     </p><li>
    Decrease the execution time attributable to computation
     <p>
     </p></li><li>
    But also, increase the execution time attributable to communication
     </li></ul>
<p>
</p></li><li> The time required for communication is dependent upon a given system's
     communication bandwidth parameters.
<p>
</p></li><li> For example, the time (t) required to send W words between any two 
     processors is:
<pre><b>
     t  =  L  +  W/B
</b></pre>
    where L = latency and B = hardware bitstream rate in words per second.
    <p>
    Latency can be thought of as the time required to send a zero byte message
</p><p>
</p></li><li> Communication patterns also affect the computation to communication ratio.
     <p>
     For example, gather-scatter communications between a single processor
     and N other processors will be impacted more by an increase in latency than
     N processors communicating only with nearest neighbors.
</p></li></ul>



<a name="io patterns"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Design and Performance Considerations
</b></font></td><td bgcolor="navy" align="right">
<a href="#bandwidth">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Communication Patterns and Bandwidth"></a>
<a href="#debugging">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Debugging"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
I/O Patterns
</b></font></td></tr></tbody></table>

<p>
</p><ul>
<li> I/O operations are generally regarded as inhibitors to parallelism 
<p>
</p></li><li> Parallel I/O systems are as yet, largely undefined and not available
<p>
</p></li><li> In an environment where all processors see the same filespace, write
     operations will result in file overwriting
<p>
</p></li><li> Read operations will be affected by the fileserver's ability to handle
     multiple read requests at the same time
<p>
</p></li><li> I/O which must be conducted over the network (non-local) can cause
     severe bottlenecks
<p>
</p></li><li> Some options:
     <ul>
     <p>
     </p><li> Reduce overall I/O as much as possible
     <p>
     </p></li><li> Confine I/O to specific serial portions of the job
     <p>
     </p></li><li> For example, Task 1 could read an input file and then communicate
          required data to other tasks.  Likewise, Task 1 could perform
          write operation after receiving required data from all other tasks.
     <p>
     </p></li><li> Create unique filenames for each tasks' input/output file(s)
     <p>
     </p></li><li> For distributed memory systems with shared filespace, perform I/O in 
          local, non-shared filespace
     <p>
     </p></li><li>
For example,  each processor may have /tmp filespace which can used.    
       This is usually much more efficient than performing I/O over the 
         network to one's home directory.      </li></ul>
</li></ul>



<a name="debugging"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Design and Performance Considerations
</b></font></td><td bgcolor="navy" align="right">
<a href="#io%20patterns">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to I/O Patterns"></a>
<a href="#performance">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Performance Monitoring and Analysis"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Debugging 
</b></font></td></tr></tbody></table>

<p>
</p><ul>
<li> Debugging parallel programs is significantly more of a challenge than
     debugging serial programs
<p>
</p></li><li> Parallel debuggers are beginning to become available, but much work 
     remains to be done
<p>
</p></li><li> Use a modular approach to program development
<p>
</p></li><li> Pay as close attention to communication details as to computation details
</li></ul>



<a name="performance"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Design and Performance Considerations
</b></font></td><td bgcolor="navy" align="right">
<a href="#debugging">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Debugging"></a>
<a href="#loop%20parallelism">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Parallel Examples"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Performance Monitoring and Analysis 
</b></font></td></tr></tbody></table>

<p>
</p><ul>
<li> As with debugging, monitoring and analyzing parallel program execution
     is significantly more of a challenge than for serial programs
<p>
</p></li><li> A number of parallel tools for execution monitoring and program analysis 
     are available
<p>
</p></li><li> Some are quite useful;  some are cross-platform also
<p>
</p></li><li> Work remains to be done, particularly in the area of scalability.
</li></ul>



<a name="parallel examples"> <br><br>   </a>
<a name="loop parallelism">   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Parallel Examples
</b></font></td><td bgcolor="navy" align="right">
<a href="#performance">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Performance Monitoring and Analysis"></a>
<a href="#pi1">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Calculating PI"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Essentials of Loop Parallelism 
</b></font></td></tr></tbody></table>

<p>
Some examples will help illustrate the methods of parallel
programming and the performance issues involved.

</p><p>
Each of the problems has a main loop.  Loops are a main target
for parallelizing and vectorizing code.  A program often spends
much of its time in loops.  When it can be done, parallelizing 
these sections of code can have dramatic benefits.

</p><p>
A step-wise refinement procedure for developing the parallel algorithms
will be employed. An initial solution for each problem will be presented
and improved by considering performance issues.

</p><p>
Pseudo-code will be used to describe the solutions. The solutions will
address the following issues:
</p><ul>
<p>
</p><li> identification of parallelism
<p>
</p></li><li> program decomposition
<p>
</p></li><li> load balancing (static vs. dynamic)
<p>
</p></li><li> task granularity in the case of dynamic load balancing
<p>
</p></li><li> communication patterns - overlapping communication and computation
</li></ul>
<p>



<a name="pi1"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Parallel Examples
</b></font></td><td bgcolor="navy" align="right">
<a href="#loop%20parallelism">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Essentials of Loop Parallelism"></a>
<a href="#array1">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Calculating Array Elements"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Calculating PI</b></font><p>
<font size="+2" face="helvetica"><b>Serial Problem Description
</b></font></p></td></tr></tbody></table>

</p><ul>
<p>
</p><li> Embarrassingly parallel
     <ul>
     <li> Computationally intensive
     </li><li> Minimal communication
     </li><li> Minimal I/O
     </li></ul>
<p>
</p></li><li> The value of PI can be calculated in a number of ways, many of which
     are easily parallelized
<p>
</p></li><li> Consider the following 
    <a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/pi.gif" target="W2" onclick="blur()">method of approximating 
    PI</a>
<ol>
   <li> Inscribe a circle in a square
   </li><li> Randomly generate points in the square
   </li><li> Determine the number of points in the square that are also in the circle
   </li><li> Let r be the number of points in the circle divided by the number of
        points in the square
   </li><li> PI ~ 4 r
   </li><li> Note that the more points generated, the better the approximation
   </li></ol>
<p>
</p></li><li> Serial pseudo code for this procedure:
<pre> npoints = 10000
 circle_count = 0

 do j = 1,npoints
   generate 2 random numbers between 0 and 1
   xcoordinate = random1 ; ycoordinate = random2
   if (xcoordinate, ycoordinate) inside circle
   then circle_count = circle_count + 1
 end do

 PI = 4.0*circle_count/npoints
</pre>
<p>
</p></li><li> Note that most of the time in running this program would be
     spent executing the loop
</li></ul>


<a name="pi2"></a>   <p></p><hr><p> 
<font size="+2" face="helvetica"><b>
Calculating PI<br>
Parallel Solution: Message Passing 
</b></font>

</p><ul>
<p>
</p><li> Parallel strategy: break the loop into portions which can be
     executed by the processors.
<p>
</p></li><li> For the task of approximating PI: 
     <ul>
     <li> each processor executes its portion of the loop a number of
          times.
     </li><li> each processor can do its work without requiring any information
          from the other processors (there are no data dependencies). This
          situation is known as Embarassingly Parallel.
     </li><li> uses SPMD model.  One process acts as master and collects
          the results.
     </li></ul>
<p>
</p></li><li> Message passing pseudo code:<br>
     <font color="red">Red</font> highlights changes for Message Passing.
<pre> npoints = 10000
 circle_count = 0
 <font color="red">p = number of processors </font>
 <font color="red">num = npoints/p

 find out if I am MASTER or WORKER </font>

 do j = 1,<font color="red">num </font>
   generate 2 random numbers between 0 and 1
   xcoordinate = random1 ; ycoordinate = random2
   if (xcoordinate, ycoordinate) inside circle
   then circle_count = circle_count + 1
 end do

 <font color="red">if I am MASTER

   receive from WORKER their circle_counts
   compute PI (use MASTER and WORKER calculations)

 else if I am WORKER

   send to MASTER circle_count
 </font>

 endif
</pre>
</li></ul>



<a name="array1"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Parallel Examples
</b></font></td><td bgcolor="navy" align="right">
<a href="#pi1">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Calculating PI"></a>
<a href="#heat1">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Simple Heat Equation"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Calculating Array Elements </b></font><p>
<font size="+2" face="helvetica"><b>Serial Problem Description
</b></font></p></td></tr></tbody></table>

<ul>
<p>
</p><li> This example shows calculations on array elements that require
     very little communication.
<p>
</p></li><li> Elements of 2-dimensional array are calculated.
<p>
</p></li><li> The calculation of elements is independent of one another -
     leads to 
     <a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/embarrasing.gif" target="W2" onclick="blur()">embarassingly 
     parallel</a> situation.
<p>
</p></li><li> The problem should be computationally intensive.
<p>
</p></li><li> Serial code could be of the form:
<pre>     do j = 1,n
     do i = 1,n
       a(i,j) = fcn(i,j)
     end do
     end do
</pre>
<p>
</p></li><li> The serial program calculates one element at a time in the specified
     order.
</li></ul>



<a name="array2"></a>   <p></p><hr><p> 
<font size="+2" face="helvetica"><b>
Calculating Array Elements <br>
Parallel Solution: Message Passing
</b></font>

</p><ul>
<p>
</p><li> Arrays are distributed so that each processor owns a portion of an array.
<p>
</p></li><li> Independent calculation of array elements insures no
     communication amongst processors is needed.
<p>
</p></li><li> Distribution scheme is chosen by other criteria, e.g. unit stride 
     through arrays.
<p>
</p></li><li> Desirable to have unit stride through arrays, then the
     choice of a distribution scheme depends on the programming language.
     <ul>
     <li> Fortran: block cyclic distribution
     </li><li> C: cyclic block distribution
     </li></ul>
<p>
</p></li><li> After the array is distributed, each processor 
     executes the portion of the loop corresponding to the data it owns.
<p>
</p></li><li> Notice only the loop variables are different from the serial 
     solution.
<p>
</p></li><li> For example, with Fortran and a block cyclic distribution:
<pre>     do j = mystart, myend
     do i = 1,n
       a(i,j) = fcn(i,j)
     end do
     end do
</pre>
<p>
</p></li><li> Message Passing Solution:
     <ul>
     <p>
     </p><li> With Fortran storage scheme, perform block cyclic
          distribution of array.
     <p>
     </p></li><li> Implement as SPMD model.
     <p>
     </p></li><li> Master process initializes array, sends info to worker
          processes and receives results.
     <p>
     </p></li><li> Worker process receives info, performs its share of
          computation and sends results to master.
     <p>
     </p></li><li> Pseudo code solution:<br>
          <font color="red">Red</font> highlights changes for Message
          Passing.
     <pre>    <font color="red">find out if I am MASTER or WORKER
   
    if I am MASTER
   
      initialize the array
      send each WORKER info on part of array it owns
      send each WORKER its portion of initial array
   
      receive from each WORKER results 
   
    else if I am WORKER
      receive from MASTER info on part of array I own
      receive from MASTER my portion of initial array
    </font>
   
      # calculate my portion of array
      do j = <font color="red">my first column,my last column </font>
      do i = 1,n
        a(i,j) = fcn(i,j)
      end do
      end do
   
      <font color="red">send MASTER results </font>
    endif
     </pre>
     </li></ul>
</li></ul>




<a name="array3"></a> <p></p><hr><p> 
<font size="+2" face="helvetica"><b>
Calculating Array Elements <br>
Parallel Solution: Pool of Tasks
</b></font>

</p><p>
</p><ul>
<li> We've looked at problems that are static load balanced. 
     <ul>
     <p>
     </p><li> each processor has fixed amount of work to do 
     <p>
     </p></li><li> may be significant idle time for faster or more lightly loaded 
          processors.
     </li></ul>
<p>
</p></li><li> Usually is not a major concern with dedicated usage. i.e. 
     loadleveler.  
<p>
</p></li><li> If you have a load balance problem, you can use a "pool of tasks"
     scheme.
<p>
</p></li><li> Two processes are employed
     <ul>
     <p>
     </p><li> Master Process: 
          <ul>
          <li> holds pool of tasks for worker processes to do
          </li><li> sends worker a task when requested
          </li><li> collects results from workers
          </li></ul>
     <p>
     </p></li><li> Worker Process: repeatedly does the following
          <ul>
          <li> gets task from master process
          </li><li> performs computation 
          </li><li> sends results to master
          </li></ul>
     </li></ul>
<p>
</p></li><li> Worker processes do not know before runtime which portion of array
     they will handle or how many tasks they will perform.
<p>
</p></li><li> The fastest process will get more tasks to do. <br>
     Dynamic load balancing occurs at run time.
<p>
</p></li><li> Solution:
     <ul>
     <li> Calculate an array element
     </li><li> Worker process gets task from master, performs work, sends
          results to master, and gets next task
     </li><li> Pseudo code solution:<br>
          <font color="red">Red</font> highlights changes for Message
          Passing.
     <pre> <font color="red">
 find out if I am MASTER or WORKER

 if I am MASTER

    do until no more jobs
      send to WORKER next job
      receive results from WORKER
    end do

    tell WORKER no more jobs

 else if I am WORKER

    do until no more jobs
      receive from MASTER next job
 </font>
      
      calculate array element: a(i,j) = fcn(i,j)

      <font color="red">send results to MASTER
    end do

 endif
 </font>
</pre>
     </li></ul>
</li></ul>


<a name="array4"></a> <p></p><hr><p> 
<font size="+2" face="helvetica"><b>
Calculating Array Elements <br>
Load Balancing and Granularity 
</b></font>

</p><ul>
<p>
</p><li> Static load balancing can result in significant idle time for
     faster processors.
<p>
</p></li><li> Pool of tasks offers a potential solution - the faster processors
     do more work.
<p>
</p></li><li> In the pool of tasks solution, the workers calculated array elements,
     resulting in
     <ul>
     <li> optimal load balancing: all processors complete work at the
          same time
     </li><li> fine granularity: small unit of computation, master and worker
          communicate after every element
     </li><li> fine granularity may cause very high communications cost
     </li></ul>
<p>
</p></li><li> Alternate Parallel Solution:
     <ul>
     <li> give processors more work - columns or rows rather than elements
     </li><li> more computation and less communication results in larger
          granularity
     </li><li> reduced communication may improve performance
     </li></ul>
</li></ul>



<a name="heat1"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Parallel Examples
</b></font></td><td bgcolor="navy" align="right"> <a href="#array1"> <img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Calculating Array Elements"></a> <a href="#application%20case%20study"> 
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to Application Case Study"></a> <!-- broken -->
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Simple Heat Equation</b></font><p>
<font size="+2" face="helvetica"><b>Serial Problem Description
</b></font></p></td></tr></tbody></table>

<ul>
<p>
</p><li> Most problems in parallel computing require communication among
     the processors.
<p>
</p></li><li> Common problem requires communication with "neighbor" processor.
<p>
</p></li><li> The heat equation describes the temperature change over time,
     given initial temperature distribution and boundary conditions.
<p>
</p></li><li> A finite differencing scheme is employed to solve the 
     heat equation numerically on a square region.
<p>
</p></li><li> The <a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/initial.gif" target="W2" onclick="blur()">initial 
     temperature</a> is zero on the boundaries and high in the middle.
<p>
</p></li><li> The boundary temperature is held at zero.
<p>
</p></li><li> For the fully explicit problem, a time stepping algorithm is used.
     The elements of a 2-dimensional array represent the temperature at
     points on the square.
<p>
</p></li><li> The calculation of an element is dependent on <a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/equation.gif" target="W2" onclick="blur()">neighbor element 
     values.</a>
<p>
</p></li><li> A serial program would contain code like:
<pre>do iy = 2, ny - 1
do ix = 2, nx - 1
  u2(ix, iy) =
    u1(ix, iy)  +
      cx * (u1(ix+1,iy) + u1(ix-1,iy) - 2.*u1(ix,iy)) +
      cy * (u1(ix,iy+1) + u1(ix,iy-1) - 2.*u1(ix,iy))
end do
end do
</pre>
</li></ul>



<a name="heat2"></a>   <p></p><hr><p> 
<font size="+2" face="helvetica"><b>
Simple Heat Equation<br>
Parallel Solutions
</b></font>

</p><ul>
<p>
</p><li> Arrays are 
     <a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/partitioned.gif" target="W2" onclick="blur()">distributed</a> 
     so that each 
     processor owns a <a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/single.gif" target="W2" onclick="blur()">
     portion of the arrays.</a>
<p>
</p></li><li> Determine data dependencies
     <ul>
     <li><a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/interior.gif" target="W2" onclick="blur()">interior 
         elements</a> belonging to a processor are independent of other 
         processors'
     </li><li> <a href="http://www.mhpcc.edu/training/workshop/parallel_intro/gif/edge.gif" target="W2" onclick="blur()">border elements</a>
          are dependent upon
          a neighbor processor's data, communication is required.
     </li></ul>
</li></ul>


<p></p><hr><p>
<font size="+2" face="helvetica"><b>
Simple Heat Equation<br>
Parallel Solution: Message Passing
</b></font>

</p><p>
</p><ul>
<li> First Parallel Solution:
     <ul>
     <li> Fortran storage scheme, block cyclic distribution
     <p>
     </p></li><li> Implement as SPMD model
     <p>
     </p></li><li> Master process sends initial info to workers, checks
          for convergence and collects results
     <p>
     </p></li><li> Worker process calculates solution, communicating as necessary with
          neighbor processes
     <p>
     </p></li><li> Pseudo code solution:<br>
          <font color="red">Red</font> highlights changes for Message
          Passing.

     <pre> <font color="red">
 find out if I am MASTER or WORKER

 if I am MASTER
   initialize array
   send each WORKER starting info
   
   do until all WORKER converge
     gather from all WORKER convergence data
     broadcast to all WORKER convergence signal
   end do

   receive results from each WORKER

 else if I am WORKER
   receive from MASTER starting info

   do until solution converged
      update time
      send neighbors my border info
      receive from neighbors their border info
 </font>

      update my portion of solution array
      <font color="red">
      determine if my solution has converged
      send MASTER convergence data
      receive from MASTER convergence signal
   end do
 
   send MASTER results
      
 endif
 </font>
     </pre>
     </li></ul>
</li></ul>




<a name="heat3"></a>   <p></p><hr><p> 
<font size="+2" face="helvetica"><b>
Simple Heat Equation<br>
Overlapping Communication and Computation 
</b></font>

</p><p>
</p><ul>
<li> Previous examples used blocking communications, which waits for the 
     communication process to complete.
<p>
</p></li><li> Computing times can often be reduced by using non-blocking
     communication.
<p>
</p></li><li> Work can be performed while communication is in progress.
<p>
</p></li><li> In the heat equation problem, neighbor processes communicated border
     data, then each process updated its portion of the array.
<p>
</p></li><li> Each process could update the interior of its part of the solution
     array while the communication of border data is occurring, and
     update its border after communication has completed.
<p>
</p></li><li> Pseudo code for second message passing solution:<br>
     <font color="red">Red</font> highlights changes for Non-Blocking
     Solution.

<pre> find out if I am MASTER or WORKER
 
 if I am MASTER
   initialize array
   send each WORKER starting info
    
   do until all WORKER converge
     gather from all WORKER convergence data
     broadcast to all WORKER convergence signal
   end do
 
   receive results from each WORKER
 
 else if I am WORKER
   receive from MASTER starting info
 
   do until solution converged
     update time
     <font color="red">
     non-blocking send neighbors my border info
     non-blocking receive neighbors border info
 
     update interior of my portion of solution array
     wait for non-blocking communication complete
     update border of my portion of solution array
     </font>
 
     determine if my solution has converged
     send MASTER convergence data
     receive from MASTER convergence signal
   end do
  
   send MASTER results
       
 endif
</pre>
</li></ul>



<a name="application_case_study"> <br>
<br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
Parallel Examples
</b></font></td><td bgcolor="navy" align="right">
<a href="#heat1">
<img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Simple Heat Equation"></a>
<a href="#References">
<img src="IntroPara_files/arrowDown.gif" width="23" height="18" border="0" alt="Down to References and More Information"></a>
</td></tr><tr><td colspan="2"><font size="+2" face="helvetica"><b>
Application Case Study 
</b></font></td></tr></tbody></table>


<p>
A detailed case study of a Numeric Weather Prediction Model, developed by
Glenn Wightwick of IBM Australia Science &amp; Technology is available
<a href="http://www.mhpcc.edu/training/workshop/parallel_intro/nwp_case_study.html" target="W2" onclick="blur()">HERE. </a>





<a name="References"> <br><br>   </a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="navy"><font color="white" size="+2" face="helvetica"><b>
References and More Information
</b></font></td><td bgcolor="navy" align="right"> <a href="#application%20case%20study"> <img src="IntroPara_files/arrowUp.gif" width="23" height="18" border="0" alt="Up to Application Case Study"></a> <!-- broken -->
</td></tr></tbody></table>

</p><p>
</p><ul>
<li> "IBM AIX Parallel Environment Application Development, Release 1.0", IBM
      Corporation. 
<p>
</p></li><li> Carriero, Nicholas and Gelernter, David, "How to Write Parallel Programs -
     A First Course".  MIT Press, Cambridge, Massachusetts.
<p>
</p></li><li> Dowd, Kevin, High Performance Computing", O'Reilly &amp; Associated, Inc.,
     Sebastopol, California.
<p>
</p></li><li> Hockney, R.W. and Jesshope, C.R., "Parallel Computers 2",Hilger, Bristol
     and Philadelphia.
<p>
</p></li><li> Ragsdale, Susan, ed., "Parallel Programming", McGraw-Hill, Inc., New York.
<p>
</p></li><li> Chandy, K. Mani and Taylor, Stephen, "An Introduction to Parallel
     Programming", Jones and Bartlett, Boston
<p>
</p></li><li> We gratefully acknowledge John Levesque of Applied Parallel Research for
     the use of his "A Parallel Programming Workshop" presentation materials.
<p>
</p></li><li> We also gratefully acknowledge the Cornell Theory Center, Ithaca, New
     York for the use of portions of their "Parallel Processing" and "Scalable
     Processing" presentation materials. 
<p>
</p></li><li> We also gratefully acknowledge Glenn Ozaki of Silicon Graphics
     Incorporated for providing the information for the Multiprocessor
     Architecture table.
</li></ul>



<script language="javascript">PrintFooter()</script><p></p><hr><font size="2"><i><br>     <a href="http://www.mhpcc.edu/">Maui High Performance Computing Center</a>     All rights reserved.<br>http://www.mhpcc.edu/training/workshop/parallel_intro/MAIN.html<br>Last Modified: Wed, 19 Dec 2001 23:25:18 GMT <a href="mailto:webmaster@mhpcc.edu">webmaster@mhpcc.edu</a></i></font><p>


<br> <br> <br> <br> <br>
<br> <br> <br> <br> <br>
<br> <br> <br> <br> <br>

</p></body></html>