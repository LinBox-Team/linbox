\documentclass{acm_proc_article-sp}
%\documentclass[final]{proceedings}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{multicol}
%\usepackage{subeqn}
\usepackage{url}

%%%%%%%%%%%%
% material borrowed form nsf/linalg05 in futile attempt to define \newblock
%\documentclass[12pt]{article}
%\usepackage{nsf} %%% Erich's NSF proposal style (itr/nsf.sty)
%\usepackage{mathptm} %%% postscript times; does not screw up pdf formats
%\usepackage{newlfont}
%\usepackage{xspace}
%\usepackage{array}
%\usepackage{amsmath}
%\usepackage{amsfonts}
%\usepackage{longtable}
%\usepackage{rotating}
%%%%%%%%%%%%

%\usepackage{crop}
%\crop
%\makeindex

\usepackage{natbib}
\bibpunct[,]{[}{]}{,}{a}{}{,}

\input{defs}

\begin{document}

%
% --- Author Metadata here ---
\conferenceinfo{ISSAC}{'05, KLMM, Chinese Academy of Sciences, Beijing, China}
%\setpagenumber{50}
%\CopyrightYear{2005} % Allows default copyright year (2002) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (X-XXXXX-XX-X/XX/XX) to be over-ridden.
% --- End of Author Metadata ---

\title{Positive Semi-definiteness and Signature of Symmetric Rational Matrices 
Arising in Lie Group Representations
%\titlenote{(Produces the permission block, copyright information and page numbering). %For use with ACM\_PROC\_ARTICLE-SP.CLS V2.6SP. Supported by ACM.
%}
}
\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}
}
%
% You need the command \numberofauthors to handle the "boxing"
% and alignment of the authors under the title, and to add
% a section for authors number 4 through n.
%
% Up to the first three authors are aligned under the title;
% use the \alignauthor commands below to handle those names
% and affiliations. Add names, affiliations, addresses for
% additional authors as the argument to \additionalauthors;
% these will be set for you without further effort on your
% part as the last section in the body of your article BEFORE
% References or any Appendices.

\numberofauthors{3}
%
% You can go ahead and credit authors number 4+ here;
% their names will appear in a section called
% "Additional Authors" just before the Appendices
% (if there are any) or Bibliography (if there
% aren't)

% Put no more than the first THREE authors in the \author command
\author{
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
%% e-mail address with \email.
\alignauthor Jeffrey Adams\titlenote{Supported by ..}\\
       \affaddr{Department of Mathematics}\\
       \affaddr{University of Maryland}\\
       \affaddr{College Park, MD, 20742 USA}\\
       \email{jda@math.umd.edu}
\alignauthor B.David Saunders\titlenote
{Supported by NSF grants 0098284 and 0112807.}\\
       \affaddr{Department of Computer and Information Sciences}\\
       \affaddr{University of Delaware}\\
       \affaddr{Newark, DE, 19716 USA}\\
       \email{saunders@cis.udel.edu}
\alignauthor Zhendong Wan\titlenote
{Supported by NSF grant 0098284 or 0112807.}\\
       \affaddr{Department of Computer and Information Sciences}\\
       \affaddr{University of Delaware}\\
       \affaddr{Newark, DE, 19716 USA}\\
       \email{wan@cis.udel.edu}
}
\date{14 January 2005}
\maketitle
\input{abstract}

% A category with the (minimum) three required fields
\category{G.4}{Mathematical Software}{Algorithm Design and Analysis}
%A category including the fourth, optional field follows...
%\category{J.2}{Physical Sciences and Engineering}[Mathematics]
\category{F.2.2}{Analysis of Algorithms and Problem Complexity}
{Nonnumerical Algorithms and Problems}[Computations on discrete structures]

\terms{Algorithms, Performance}

\keywords{
matrix signature, symmetric matrix, lie group}
% NOT required for Proceedings

\section{Introduction}
In the study of Lie group representations it is helpful to determine key
properties of certain operators, which in the matrix representations are
dense symmetric integer or rational matrices.  At issue is to compute, 
for certain symmetric operators arising in the representation, whether they
are positive definite, semidefinite, or have other signature of
eigenvalue signs.  

These operators may be represented as a product
of quite sparse matrices or expanded as a single dense matrix.
Thus we may consider both dense matrix eliminations and blackbox algorithms, 
in which the matrix is kept in product form and only matrix vector products are used.
For example, the largest matrix for which signature information is sought
is a product of 121 sparse rational $7168\times 7168$ matrices having a total of 
x? nonzero entries and no numerator or denominator larger than y?.  
When the product is expanded, the 
matrix entries have several hundred digits and the whole matrix occupies over 
17GB of memory (well more than can be referenced with 32 bit addresses).  
Working with this matrix seems to be just at the frontier of feasability.
We are also interested in other operators, and representation in models of lower dimension,
where the scale is not so extreme.

This is an experimental paper.  Our goal is to assess the suitability
of several algorithms which we describe by demonstrating their performance on 
smaller examples and assessing their prospects for use on the larger matrices.
Along the way we do record a few useful facts concerning asymptotic complexity
and probabilities of randomized methods. (fixme: revisit this comment)

Let A be a symmetric integer or rational matrix.
The eigenvalues of A are real.  The signature of A is the 
triple $(\p, \z, \n)$, where $\p$ is the number of positive eigenvalues, $\z$ is the 
multiplicity of zero as an eigenvalue, and $\n$ is the number negative.
A is 
positive definite if the signature is $(*, 0, 0)$, 
positive semidefinite if the signature is $(*, *, 0)$, 
negative definite if the signature is $(0, 0, *)$, 
negative semidefinite if the signature is $(0, *, *)$, 
and indefinite if the signature is $(+, *, +)$, 
where $*$ denotes an arbitrary value and $+$ denotes a positive value.
We will use this notation for describing signature patterns throughout the paper.

We propose, analyze, and test several algorithms for 
computing the signature and for computing signature patterns (hence 
definiteness properties).  The latter is sometimes easier than
the signature.  Also there is often a considerable difference in the 
cost of verifying (certifying) a property and of certifying it's negation.

In section 2, the motivation for this work in the study of Lie Group
representations is sketched.  This application creates large problems
straining our limits of time and memory.

In section 3 we establish the mathematical foundations for the algorithms
which are presented and analyzed in section 4, including some discussion
of unimplemented alternatives and of the space issues for large instances
of the problems.  Some quick correctness checks of model and operator 
construction are also presented.

Finally, in section 5, the experimental results are reported
and, in section 6, conclusions drawn on the state of this problem.

\input{liegroups} % Jeff

\section{Algorithm foundations} %section 3

For the rest of this paper
We consider the general question of signature and signature pattern
of symmetric matrices, but constantly keeping in mind the specific cases of 
operators that may be generated from Lie group matrix representations.  In 
particular, we refer often to $\M[n,f]$, the operator generated from facet $f$
in the model of dimension $n$.  The construction of $M$ also depends on the 
specific model of dimension $n$ and specific root system.  For present purposes
fix the root system as that for group E8 given at the \cite{lieatlas} site
and the model is also given there.  For example Q112Cpt at \cite{lieatlas}
gives the generators of the representation of E8 of dimension 7168 (the largest).
Most specifically, we seek to verify or disprove the positive definiteness
of $\M[n,1/40(0,1,2,3,4,5,6,7,23)].

% this should go after signature?
The following theorem summarizes some useful standard properties of symmetric
matices vis a vis definiteness and semidefiniteness.

\begin{theorem}
Let $A$ be a real symmetric $n\times n$ matrix. Then TFAE:
\begin{enumerate}
\item
$A$ is positive definite.
\item
For all $n$-vectors $x, x^T A x > 0$. [usual definition of positive definite]
\item
All eigenvalues of $A$ are positive. [our definition in introduction]
\item
All principal minors of $A$ (of all sizes) are positive.
\item
The $i\times i$ leading principal minor of $A$ is positive, for each $ i \in 1..n$.
\end{enumerate}
\end{theorem}

\begin{theorem}
Let $A$ be a real symmetric $n\times n$ matrix. Then TFAE:
\begin{enumerate}
\item
$A$ is positive semidefinite.
\item
For all $n$-vectors $x, x^T A x >= 0$. [usual definition of positive definite]
\item
All eigenvalues of $A$ are nonnegative. [our definition in introduction]
\item
All principal minors of $A$ (of all sizes) are nonnegative.
\item
The $i\times i$ leading principal minor of $A$ is nonnegative, for each $ i \in 1..n$,
and, whenever the leading $i\times i$ principal minor is zero, then every minor in the first $i$ rows is zero, i.e. if
\[
0 = A\left(\begin{array}{lll}
1\mbox{,} & \ldots\mbox{,}& i\\1\mbox{,} &\ldots\mbox{,}& i \end{array}\right)
\mbox{then}
0 = A\left(\begin{array}{lll}
1\mbox{,} & \ldots\mbox{,}& i\\j_1\mbox{,} &\ldots\mbox{,}& j_i \end{array}\right),
\]
for each sequence $(j_1, \ldots, j_i)$.
\end{enumerate}
\end{theorem}

% ? Put here a theorem about signature and diagonal of LDL^T?

%of the signature, and sometimes is and definiteness properties of 
%the signs of the 
%(real) eigenvalues 
%We begin with definitions of signatures and definiteness properties.
%As the eigenvalues of a matrix are the roots of its characteristic 
%polynomial, it is useful to start with 
%the corresponding properties of polynomials.
%Let $f(x)$ be a polynomial of degree $n$ with $n$ real roots.
%We define the {\em signature} of $f$ as the triple $s = (\p, \z, \n)$, where the
%three entries are, respectively, the number of positive, zero, and negative roots.
%Then if $A$ is a matrix with only real eigenvalues, it's {\em signature} may be defined as the signature 
%of it's characteristic polynomial.  
%Thus the signature of a symmetric matrix $A$ can
%be determined by computing $\charpoly(A)$ as discussed in detail
%in the next section.

%For definiteness properties it suffices to know the pattern of a signature.
%The sign pattern of a signature, is the signature triple with each positive 
%entry replaced by 1.
%For example, the pattern of signature (143, 291, 0) is (1, 1, 0).
%
%The definiteness properties of a symmetric matrix $A$ may be defined 
%in terms of the sign pattern of the characteristic polynomial of $A$ 
%as follows ($*$ denotes an unspecified value):
%
%The matrix is {\em positive definite}, if the pattern is (1, 0, 0).\\ 
%The matrix is {\em positive semidefinite}, if the  pattern is (1, *, 0).\\ 
%The matrix is {\em negative definite}, if the  pattern is (0, 0, 1).\\ 
%The matrix is {\em negative semidefinite}, if the  pattern is (0, *, 1).\\ 
%The matrix is {\em indefinite}, if the  pattern is (1, *, 1).\\ 
%% The matrix is {\em singular}, if the  pattern is (*, 1, *).\\ 
%

Let us define the {\em signature} of a real polynomial which has only real roots 
to be the triple of positive, zero, and negative roots.  Thus the signature of
a real symmetric matrix is the signature of its characteristic polynomial.
The definiteness properties of a symmetric matrix $A$ are determined by 
the sign pattern of the signature of $A$.  It is 
not necessary to have the exact count of roots in each category.
Note that the 
sign patterns of $\charpoly(A)$ and $\minpoly(A)$ are the same,  
since every eigenvalue of $A$ occurs as a root of $\minpoly(A)$. 
Thus for definiteness determinations, it suffices to compute $\minpoly(A)$, 
In some cases the minimal polynomial is much easier to compute.

If it is only required to distinguish positive definite from positive semidefinite,
(more generally nonsingular from singular), it suffices to determine if the 
constant coefficient of the minimal polynomial is zero. This may be done
particularly rapidly, probabilistically. 
Let the test of singularity be to compute the minimal polynomial mod a single
randomly chosen prime and checking if zero is a root, i.e. if the constant coefficient
is zero.
This test gives no false positives for singularity, because 
singularity is preserved mod any prime and the probabilistic minimal polynomial algorithm
of \cite{Wied}, has as it's only failure mode that it may return a proper factor of the 
minimal polynomial.
Thus if zero is a root of the computed polynomial, the matrix is definitely singular.
Nonsingularity may not be preserved mod the prime and the minpoly factor returned could
miss an $x-0$ factor, so a false positive from this test {\em is} possible though
highly unlikely.  
It is very easy to keep the proven probability of error less than one in a billion, 
and in practice it {\em never} fails.

To be thorough, 
we next give the algorithm to compute the signature of a polynomial.
It is just an application of 
Descartes' rule of signs, which  uses counts of alternations of sign in the coefficients
of a matrix to bound the number of positive and negative roots.  
For a polynomial of degree $d$ it is just a manipulation of the sequence
of the $d+1$ coefficients, given in decreasing term order.  Later
we will also use it on sequences not arising from a polynomial.

\begin{algorithm}{Signature}\label{algo:SP}
\Inspec The sequence of coefficients $(f_d, f_{d-1},  \ldots, f_0)$ of a polynomial $f(x) = f_d x^d + \ldots$ of degree $d$.
\Outspec The signature $(\p, \z, \n)$ of $f(x)$.  \\
  $[$where $d = \p + \z + \n.]$\\ 
\Stmt[1.] [ Number of zero roots is exponent of least term.] \\
Let $\z := \min\{i : 0 \le i \le d : f_i \neq 0\}$.\\
\Stmt[2.] [ Number of positive roots is the number
of alternations of sign in the nonzero coefficients.] \\
$\p := 0.$\\
\prev := $\sign(f_k)$.\\
For $i = k+1$ to $d$, when if $f_i \neq 0$ do: \\
$~~~~$ If $\sign(f_i) \neq $ \prev, $\p := \p + 1.$\\
$~~~~$ \prev:= $\sign(f_i)$.\\
$[$Also $\n = $ alternations of sign in $f(-x)$, but unnecessary to do that here.$]$\\
\Stmt[3.] [ Negative roots and return ] \\
$\n := d - \p - \z]$.\\
Return $(\p, \z, \n)$.
\end{algorithm}

Descartes' rule of signs determines the exact count of positive roots
when all the roots are real, %\cite{something on drs}
hence the algorithm is correct.
Also, evidently, algorithm Signature runs in linear time,
provided the cost of sign determination is constant.\qed

The following theorem is helpful, particularly for blackbox matrices,
because it shows that a matrix signature may be determined via
preconditioning and a minimal polynomial computation.

\begin{theorem}
Theorem: If $A$ is an $n\times n$ real symmetric matrix and $D$ is nonsingular, 
the signatures of $A$ and $B$ = $DAD^T$ are the same.  
Furthermore, 
\begin{enumerate}
\item
If $D$ is a diagonal matrix whose $n$ diagonal entries are chosen
uniformly at random from a set of nonzero integers of cardinality $s$, 
then the probability is at least $1 - n(n-1)/s$ that, 
for some nonnegative integer $k$, $x^k\minpoly(B) = \charpoly(B)$.
\item
If $D$ is a butterfly matrix whose $n \log (n)$ diagonal entries are chosen
uniformly at random from a set of nonzero integers of cardinality $s$, 
then the probability is at least $1 - n(n-1)/s$ that, 
B has generic rank profile (leading principal minors are nonzero up to the rank).
\end{enumerate}
\end{theorem}

Proof: For the first statement, fixme we explain it or cite the literature.  
For the probability statement, see \cite[theorem 4.2]{CEKSTV02}
\QED

However,
it is not necessary to compute the characteristic or minimal 
polynomial to answer signature and definiteness questions.  
It may be faster to do a $LU$ type decomposition.  
Consider the $U$ of an $LU$ decomposition of given matrix $A$, in which $L$ is unit lower triangular and $U$ upper triangular.
Suppose that whenever a diagonal element of $U$ is zero, the entire row is zero.
$U$ may be written as $DM$, where $M$ is unit upper triangular.  
When $A$ is symmetric, $M = L^T$. See, for example, \cite{gvl?}.

A note on pivoting:
Of course not every matrix has an $LDM$ decomposition.  Pivoting may be 
necessary.  The properties we desire are preserved if symmetric pivoting
works, leading to a decomposition $A = PLDL^T P^T$ decomposition, for some
permutation matrix $P$.
However, even this is not always possible.
For instance, the matrix 
\[A = \left( \begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix} \right)\]
has no such decomposition.  

Following ,%\cite{some kaltofen paper}
we say a matrix of rank $r$ has {\em generic rank profile} if the
first $r$ leading principal minors are nonzero.
Fortunately, 
the desired property about zero rows is ensured in the case that the matrix 
has generic rank profile, a property easily achieved through preconditioning.

\begin{theorem}
Let A be a real symmetric $n\times n$ matrix.
Then 
\begin{enumerate} 
\item
If A is in generic rank profile,
it has a (unique) $LDL^T$ decomposition 
such that $L$ is unit lower triangular and $D$ is diagonal.  Moreover,
the signature of $A$ consists of 
the numbers of positive, zero, and negative diagonal elements of $D$.
\item 
Let $D_i$ denote the $i$-th leading principal minor of $A$, and 
let $f(x)$ be the polynomial having coefficient sequence
$(1, D_1, \ldots, D_n)$.  
% optional
%Thus, $f(x) = x^n + \sum_{i=1}^n D_i x^{n-i}$. 
If A is in generic rank profile,
the signature of $f$ equals the signature of $A$.
\item
If $B$ is a random nonsingular matrix, with high probability,
$BAB^T$ has generic rank profile.  
Fixme quantify (B can be Toeplitz or butterfly).
\end{enumerate}
\end{theorem}

Proof: 
1.
If A is symmetric of rank r with leading r by r minor nonzero,
it has a decomposition as
%A = B C'
%    C E, where E = CB^C' (Shur complement is zero)
%Az = lambda z  bx +bc'y = x, cbx +cbc'y = y (hence cx = y)
%so  
%L     0 D 0 L' D`L`'C' = LD 0 L' X' = LDL'     LD(CL`D`)'     = B C'   
%CL`D` I 0 0 0     I       XD 0 0  I   CL`D`DL' CL`D`D(CL`D~)'   C E.

2. 
The diagonal entries of the LDL' decomposition are quotients of successive
leading principal minors, \cite[page 298?]{gantmacher}. 

3. \cite{CEKSTV02, KS91} % other butterfly of turner?
\qed


\section{Algorithms} %section 4
We developed and tested three algorithms for determining the signature
of a symmetric integer or rational matrix.  These are the best candidates
we know of for the problem.  
Each has the potential to perform best for some range of input matrices.
We discuss each method in detail in this section showing that the
first can be asymptotically better or worse than the other two, which
differ only by a constant factor asymptotically, as a function of input 
matrix order and entry size.

All of these algorithms avoid expression swell, by working mod a series
of primes and using the Chinese Remainder Theorem.  The sizes of the 
intermediate numbers would be prohibitive without modular computation.
As anecdotal evidence of the expression swell problem in this setting,
we remark that even the few (linear in dimension) integers 
needed for the answer (polynomial coefficients or diagonal matrix elements)
needed for the answer (polynomial coefficients or diagonal matrix elements)
are quite large.  At one point
we tried to explicitly create the operator corresponding to the facet
$1/40 \times (0,1,2,3,4,5,6,23)$ in the model of dimension 7168 for E8.
The matrix proved to require more than $2^32$ bytes of storage, so that it
doesn't even fit into a virtual memory when 32 bit addresses are used.

The algorithms use a CRA object which has the following properties.
The CRA maintains a positive integer, the current modulus, $M$, and a 
the sequence, $S$, of residues with respect to $M$.
The function step($\cra, m, s$) applies the Chinese remainder algorithm
componentwise to elements of the sequences $s$ and $S$, modifying the element
of the internal sequence $S$ to be the corresponding value mod $Mm$.
To be precise the new $S_i$ is a residue mod $Mm$ which is equal to 
$s_i \mod m$ and to the old $S_i \mod M$.  
Finally, $M$ is updated to become $Mm$.  
However, if $m$ and $M$ are not coprime, then $S$ and $M$ are not changed. 

The function deg($\cra$) returns the length of the sequence $S$.
Also, the function residues($\cra$) returns the current residue sequence $S$ 
and the function modulus($\cra$) returns the current modulus $M$.
The function reset($\cra, s, m$) abandons previous values and resets
the CRA object to $M = m$ and $S = s$.  Also, initially before any step()
or reset(), $M$ is 1 and $S$ is the empty sequence.

In support of early termination strategies, at each step $\cra$ updates
an internal number reflecting how many of the most recent steps have resulted
in no change to any of the residues.  The function stable($\cra$) returns
this count. 
For notational convenience, when using the CRA object we will identify a 
polynomial with it's sequence of coefficients.

\subsection{Signature algorithm for blackbox matrix}
In this subsection we assume the $n\times n$ matrix $A$ in question is a 
blackbox for a symmetric matrix.  
For instance it could be the Lie group element, 
product of 121 very sparse matrices, discussed above.  
More specifically, assume the blackbox matrix vector product is implemented 
using $E$ axpy calls, where $0 \leq E \leq n^2$.

\begin{algorithm}{SignatureByMinpoly}\label{sigmin}
\Inspec $A$, an integer symmetric $k\times k$ matrix.
\Outspec Signature $s = [\p, \z, \n]$ of $A$.
\Stmt[1.] $[$ Preconditioning may be necessary $]$\\
Let $q$ be a random prime.\\ 
Let $r := \rank(A,q)$ and $m_q(x) := \minpoly(A,q)$.\\ 
$[$ \rank and \minpoly can work even if $A$ is a rational matrix, 
provided $q$ does not divide any denominator of $A$.$]$\\
If $\deg(m_p) \leq r < \order(A)$, let $B := DAD$,\\
$~~~$ for a random integer matrix $D$ with entries chosen\\ 
$~~~$ from $1..s$ (or any set of $s$ nonzero integers).\\
Otherwise, let $B := A$.\\
$[$ Now B has the same signature as A and has minpoly essentially equal charpoly. ]\\
\Stmt[2.] $[$ Compute minpoly $]$\\
Repeat until stable-steps($\cra$) > threshold:\\
$~~~$ Choose a prime $p$ at random.\\
$~~~$ Let $m_p(x) := \minpoly(B, p)$. \\
$~~~$ If $\deg(m_p) = \deg(\cra)$, then\\
$~~~~~~$ step($\cra, m_p, p$)\\
$~~~$ If $\deg(m_p) > \deg(\cra)$, then [start over]\\
$~~~~~~$ reset($\cra, m_p, p)$.\\
$~~~$ If $\deg(m_p) < \deg(\cra)$, then [bad prime, skip it].\\
\Stmt[3.] $[$ Certify $]$\\
Let $m(x) =$  residues($\cra$).\\
If $\coeff(m, d-1) \neq \trace(B)$, return ``fail".\\
Otherwise, \\
$~~~$ Let $s = (\p, \z, \n) := Signature($m$)$.
$~~~$ Return $s' := (\p, k - \p - \n, \n)$.
\end{algorithm}

\begin{theorem}
Algorithm SignatureByMinpoly is a LasVegas probabilistic algorithm.
For an integer blackbox symmetric matrix, $A$,  
of order $n$, whose entries (when constructed) are of length bounded by $d$,
it runs in expected time 
$\softO(n^2 E d)$.
In particular if $E \in \softO(n)$, and $d \in \bO(\log(n))$,
then the expected run time is $\softO(n^3)$.
\end{theorem}

Proof:
The computation of $A_p$ costs at most $\softO(dE)$, since the number of stored data values
needed is at most $E$, each of length bounded by $d$.  The rank and minpoly of $A_p$ cost at most $\softO(nE)$, \cite{SSV04}, assuming a prime of length O($nd$).  

If this rank is correct, the algorithm will be correct.  If it is not,
that fact will be caught by the certification in step 3.  The probability
of error for this rank is 

Step 2 costs at most the insignificant (linear) time for the construction of $D$.  
The major cost is the $m = \minpoly(B)$ over the integers in step 3.
We assume here that integer matrix minpoly will be computed (MonteCarlo)
by a choosing a series of random primes $p$, computing the minpoly mod
these primes, and finally combining results by computing the coefficients
over the integers using the Chinese Remainder Algorithm.  The number
of primes needed according to Hadamard's bound is $\softO(n d))$.
Since the cost of a minpoly mod each prime is $\softO(nE)$, the cost of this
step is $\softO(n^2 E d)$.
In practice we use an early termination policy, terminating the series 
of modular computations when the last prime fails to cause any change in 
the residues mod the product of the primes.
The check in step 3 is essentially the rank certificate of \cite{SSV04}.
\QED

If the matrix is of rational numbers, the method may still be used.  However
the probability bounds are much poorer... fixme.


The $LU$ decomposition based signature algorithm assumes a procedure,  
$\LUdiag(A, p)$, which 
returns a vector consisting of the the diagonal of $U$ in the $LU$-decomposition of $A$.  
If there is no $LU$-decomposition for $A$, it sets the variable
$\LUfailure$ to true, otherwise false. For a vector $v$, we define 
$\deg(v)$ to be the largest index such that $v_i \neq 0$.  Begging your
pardon, the term ``degree" is used here to emphasize the parallel
between this algorithm and SignatureByMinpoly.

\begin{algorithm}{SignatureByLU}\label{siglu}
\Inspec $A$, an integer symmetric $k\times k$ matrix.
\Outspec Signature $s = [\p, \z, \n]$ of $A$.
\Stmt[1.] $[$ Preconditioning may be necessary $]$\\
Let $q$ be a random prime.\\ 
Let $v := \LUdiag(A,q)$.\\
$[$ \LUdiag can work even if $A$ is a rational matrix, 
provided $q$ does not divide any denominator of $A$.$]$\\
If \LUfailure, let $B := QAQ^T$,\\
$~~~$ for a random integer matrix $Q$ with entries chosen\\ 
$~~~$ from $1..s$ (or any set of $s$ nonzero integers).\\
$~~~$ [Q may be a Toeplitz or Butterfly matrix for speed]\\
Otherwise, let $B := A$.\\
$[$ Now B has the same signature as A and has generic rank profile. ]\\
\Stmt[2.] $[$ Lift $]$\\
Repeat until stable-steps($\cra$) > threshold:\\
$~~~$ Choose a prime $p$ at random.\\
$~~~$ Let $D_p := \LUdiag(B, p)$.\\
$~~~$ If $\deg(D_p) = \deg(\cra)$, then\\
$~~~~~~$ step($\cra, D_p, p$)\\
$~~~$ If $\deg(D_p) > \deg(\cra)$, then [start over]\\
$~~~~~~$ reset($\cra, D_p, p)$.\\
$~~~$ If $\deg(D_p) < \deg(\cra)$, then [bad prime, skip it].\\
%% $~~~$ If not \LUfailure, then step($\cra, D_p, p$)\\
%% $~~~$ If \LUfailure, [bad prime, skip it].\\
\Stmt[3.] $[$ Return $]$\\
$~~~$ Return $s = (\p, \z, \n) :=$ Signature(residues(\cra)).
\end{algorithm}

\begin{theorem}
Algorithm SignatureByLU is ..... 

Rest is from signature by minpoly...

a LasVegas probabilistic algorithm.
For an integer blackbox symmetric matrix, $A$,  
of order $n$, whose entries (when constructed) are of length bounded by $d$,
t runs in expected time 
$\softO(n^2 E d)$.
In particular if $E \in \softO(n)$, and $d \in \bO(\log(n))$,
fthen the expected run time is $\softO(n^3)$.
\end{theorem}

Proof:
The computation of $A_p$ costs at most $\softO(dE)$, since the number of stored data values
needed is at most $E$, each of length bounded by $d$.  The rank and minpoly of $A_p$ cost at most $\softO(nE)$, \cite{SSV04}, assuming a prime of length O($nd$).  

If this rank is correct, the algorithm will be correct.  If it is not,
that fact will be caught by the certification in step 3.  The probability
of error for this rank is 

Step 2 costs at most the insignificant (linear) time for the construction of $D$.  
The major cost is the $m = \minpoly(B)$ over the integers in step 3.
We assume here that integer matrix minpoly will be computed (MonteCarlo)
by a choosing a series of random primes $p$, computing the minpoly mod
these primes, and finally combining results by computing the coefficients
over the integers using the Chinese Remainder Algorithm.  The number
of primes needed according to Hadamard's bound is $\softO(n d))$.
Since the cost of a minpoly mod each prime is $\softO(nE)$, the cost of this
step is $\softO(n^2 E d)$.
In practice we use an early termination policy, terminating the series 
of modular computations when the last prime fails to cause any change in 
the residues mod the product of the primes.
The check in step 3 is essentially the rank certificate of \cite{SSV04}.
\QED

%\begin{algorithm}{LeadingPrincipalMinorsSignature}\label{algo:LPMS}
%\Inspec The leading principal minors $D_1, \ldots, D_n$ of a real symmetric matrix.
%\Outspec The signature $s = (\p, \z, \n)$ of the matrix.\\
%\Stmt[1.]
%Let $p(x)$ be the polynomial with coefficients $(1, D_1, \ldots, D_n).$\\
%Return Signature(p).
%\end{algorithm}
%
%\subsection{Remarks on Lifting versus Chinese Remaindering, and misc}
%Cholesky mod primes?

\subsection{Checking the Model}
It proved useful to do some basic checks of the models to verify
computationally that they have the intended properties.  Such checks
could catch errors in construction of the model (something we didn't 
encounter, fortunately, but can imagine) or errors in reading the data 
(something from which we did suffer). 
For instance, the defining operators may have been prepared in one format, converted 
by a program to another, tweaked manually, and finally read by 
our programs.  We had cases where file format disagreements lead to 
incorrect values but there was no program error while reading.
The corollary below establishes some quick checks of the model
and of the matrix generated from a given facet.

Order checks:
As described in section 2 the model consists of a basis of operators.
Each basic operator $X_i$ should have order 2, and the products $X_i X_j$ 
should have a specified small order determined from the Dynkin diagram.

Symmetry check:
Recall also that the operator determined by a facet and to be checked for signature 
and/or definiteness is a (dense) matrix 
generated as a product of factors determined by the (sparse) basic operators 
of the model.
This matrix should be symmetric and checking this is another basic 
confirmation of the proper reading of the model and root data, and of 
proper use of that data use in constructing the matrix.

Fixme: There should be something in here about the mapping from Q to a prime.

\begin{theorem}  If $A$ is a non-zero linear operator on vector space $V$ of dimension
n over finite field of cardinality $q$,  then, for vector $x$ chosen uniformly
at random from $V$, 
\[ 1 - 1/q \leq \P(A = 0 \cond Ax = 0).\]
\end{theorem}

Proof: If $A$ is non-zero, then $w := Ae_i \neq 0$ for some column $e_i$ of the identity.
Each entry of $v$ is uniformly distributed over $F$.  
Let $w := \sum_{j \in 1..n} v_j e_j - \sum{j \in 1..^i..n} v_j Ae_j$.
If $v = Av$, then $w = v_i Ae_i$ 
Since $Ae_1 \neq 0$, there is at most 1 value (out of $q$) for $v_1$ such that $v_1 Ae_1 = w$.

We say $A$ is of {\em order} $d$, 
if $d$ is the least postitive exponent such that $A^d = I$,
and we say $A$ is {\em locally of order} $k$ at vector $v$, 
if $k$ is the least postitive exponent such that $A^kv = v$,

\begin{corollary}
Let $A$ be an operator on $F^n$ for field $F$ of cardinality $p$ and 
and let v a uniformly distributed random vector in $F^n$.  
\begin{enumerate}
\item
Let $i$ be a positive integer, then \[1 - 1/p \leq \P(\ord(a)|i \cond A^i v = v )\].
\item
\[1 - \min(n,\mbox{primes(\ord(A))})/p \leq \P(A\mbox{ is of order }k \cond A\mbox{ is locally of order }k\mbox{ at }v,\]
where primes($d$) denotes the number of prime factors of $d$
\item
$1 - 1/p \leq \P(A \issym \cond Av = A^T v).$
\end{enumerate}
\end{corollary}

Proof: 

For 1, We have $\ord(A)|i$ if and only if $A^i - I$ is the zero matrix.
Apply the theorem.

For 2, Supose $A$ has order $d$.  Then, over an extension field if necessary, $A$ is similar to a diagonal matrix, $A = QDQ^{-1}$.   
The eigenvalues, diagonal entries of $D$, are $d$-th roots of unity.
Also $\ord(D) = \ord(A)$ and $A$ has order $k$ locally at $v$ if and only if $D$ has order
$k$ at $w := Qv$, so we may restrict to the case that $A$ is diagonal with it's eigenvalues
$\lambda_i$ as diagonal entries.  Note that they are roots of unity of orders dividing $d$.
Then $A^j\sum v_i e_i = \sum v_i \lambda_i^j e_i$ which equals $v4 if and only, for 
each $i$, either $v_i$ is zero or $\lambda_i^j = 1$.
The eigenvalues have orders which are divisors of $d := \ord(D)$.  

%....
%And $D^i w = 
%If $k$, the order of $D$ locally at $w$ is less than $d$ 
%then there must be at least one prime power $q$ in $d$ which fails to divide $k$ (although
%$k$ may contain a lower power of the prime).  Also there
%must be an eigenvalue, $d_{i,i}$, whose order is a multiple of $q$.
%$k$ is lower thantLet S be the support of w, 
%i.e. the indices such that the coefficients w_i of W in the eigenvector basis are nonzero.
%A has order d locally at w if and only if the lcm of the orders of the eigenvalues
%under support S is d.  The order at w is lower if and only if w_i is Suppose $d$ factors
%as $\prod_i p_i^{k_i}$.  There must be at least one eigenvalue lh$S$ be the set of indices $i$
%at which $w_i$ is nonzero.  The order of $D$ locally at $w$ will be the least common 
%multiple of the orders of of the eigenvalues indexed by $S$.  In the worst case, each 
%eigenvalue has a distinct prime order (and $d$ is a product of $n$ primes).  The probability
%of failure is bounded, then, by the probability of having one or more zero coefficients
%in the eigenvector basis.
%in 
%at lause
%The order of The eigenvalues of roots of unity bbbfor each $i < \ord(A)$, part 1 bounds the probability that  we
%are fooled into thinking the order is $i$.  The $\ord(A)-1$ events we consider 
%here are not independent (or are they), so how exactly do we complete this
%argument?

For 3, $A - A^T$ is the zero matrix if and only if $A$ is symmetric.
Apply the theorem.

%Recall that the operator determined by a facet and to be checked for signature 
%and/or  definiteness is a (dense) matrix 
%generated as a product of factors determined by the (sparse) basic operators 
%of the model.
%This matrix should be symmetric and checking this is another basic 
%confirmation of the proper reading of the model and root data, and of 
%proper use of that data use in constructing the matrix.
%
%Corollary:
%Let $A$ be a nxn matrix over field $F$ of cardinality $q$.
%Let $v$ be a random vector, uniformly chosen  over $F^n$.
%Then $P(A \issym \cond Av = A^T v) >= 1/q$.
% 
%Proof:  $A - A^T$ is the zero matrix if and only if $A$ is symmetric.
%Apply the theorem.

\section{Experiments} %section 5
Our goal is to know which algorithm to use and how much resourses it will take 
for computing the signature of $M_f(n)$, the matrix generated by the facet $f$
for the model of dimension $n$ of the group E8.  In particular, we focus on the case when
the facet is the challenge facet $f = 1/40 (i0,1,2,3,4,5,6,23)$, 
and the goal is to verify (or disprove) that $M_c(n)$ is positive definite.  
Our experiments are designed to give this information.

The three signature algorithms we have discussed are 
the blackbox SignatureByMinpoly the dense matrix SignatureByMinpoly,
and the dense matrix SignatureByLU, which we will call BBSM, DSM, and DSLU
respectively.  Our main goal is to predict accurately how long the larger computations
will take, given the experimental data on smaller instances and noting that computations
on $M_c(1008)$ take up to 3 days.
goal is to know which of them to use for a given signature problem
and 

Each of them involves a series of steps mod primes, each step being
a computation of a minpoly or $LDL^T$-decomposition mod the prime. 
The computation is finished when the CRA lifting operations on the vector
of minpoly coefficients or of diagonal entries
achieve a result with a modulus just larger than the largest coefficient
or LU diagonal entry needed in the integer polynomial or matrix $U$ respectively. 
Suppose $B$ bits are needed in the modulus to exceed the largest entry in this vector.
and individual primes of $s$ bit length are used.  
Let \step(s) denote the cost per step which varies with length, $s$, of the prime, 
but is essentially constant among primes of the same length.
The total time  is $B/s \times$ \step(s) with $s$ bit prime. 
Thus we see the advantage of minimizing $\step(s)/s$, the cost per bit.
After discussing $\step(s)/s$ and determining it's value, for each algorithm,  
we will want to know $B$ in order to predict the full run time for the larger instances.

\subsection{Cost per bit}
The step cost is a generally increasing or at any rate nondecreasing function of the prime length.
The optimal $s$ depends on the specific growth curve of this function.  It is somewhat 
erratic, for instance there is 
a significant jump in cost when you go beyond wordsize, $s = 32$, because
of the switch from single machine instructions to software functions for the basic add, sub,  and 
mul operations.  For this reason we only consider primes of length less than 32.  There is 
also a gain in step cost if the prime is a bit smaller, because then sums of products
such as row of matrix times vector dot products can be accumulated in 64 bit precision with
a single division with remainder to normalize mod p at the end.  The shorter the prime, the 
longer the sum that can be accumulated in this way.  See \cite{x} for further discussion.
This is particularly true when large dot products dominate as in the dense matrix codes.
We found prime lengths in the low 20s best for the dense matrix computations and primes
of around 30 bits best fo the blackbox algorithm.

We measured the cost per bit, \step(s)/s, for BBSM 
on models through the full range up to dimension 7168 and measured
the cost per bit of DSM and DSLU up to the largest size for which memory
requirement to store the full integer matrix was reasonable (i.e. ...).  
We chose not to go beyond At n = ? at which point the the memory reauirement to store
$M_c(n)$ fully expanded required xxx? megabytes.  Earlier computation showed that 
$M_c(7168)$ required in excess of 15 GB, well beyond addressability with 32 bit addresses.
Figure 1 shows
the costs.  As expected (I hypothesize) the blackbox minpoly run time
grows like $a*n^2$ (BBSM) and the eliminations grow like $b*n^3$ (DSM) and
$c*n^3$ (DSLU). 
Least squares fit of the data  gives the coefficients $a = ?, b = ?,$  and $c = ?$.
We plot those three lines in relation to the data points.
\begin{verbatim}
-----
figure:
x = log n vs y = log t 
three point symbols for bbsm dsm dslu various times.
3lines
lgt = lga + 2lgn, (for t = an^2)bb
lgt = lgb + 3lgn, (for t = bn^3)blas
lgt = lgc + 3lgn, (for t = cn^3)lu
-----
\end{verbatim}
The crossover is at about  X and one may conclude to us DSM when $n < X$ and 
BBSM for larger $n$.  
We were somewhat surprised to find $c > b$ but we note that DSLU performs it's elimination
on the symmetric matrix, $M$, itself and can reduce that cost by a factor of 2, taking advangage of 
the symmetry,. something we have not
yet implented.  On the other hand the DSM computes the minimal polynomial of $M$ by elimination 
on a Krylov matrix, $(x, Mx, M^2x, \ldots)$, where there is no such symmetry. 

\subsection{Number of bits }
We ran the full algorithms on models for E8 of various sizes, from $1\times 1$
to $1008\times 1008$.
The run times for models of these dimensions vary from a fraction of a 
second to near three days.  
We also recorded the number of primes needed for each
computation and their lentgh, so that we have the number of bits needed in these instances. 

At this time, we 
don't have theory to predict how many bits will be needed.  In both the characteristic
polynomial coefficient and $LDL^T$ diagonal, the entries lifted are minors or sums of minors.
So the Hadamard determinant bound may be used to provide an upper bound for the entry lengths.
The worst case for a $n\times n$ matrix of $d$ bit integers is $B = \O(n*(\log(n) + d)$), 
by the Hadamard determinant bound.
Rough estimation suggests that $d$ may be proportional to $\log(n)$ in these models,
Assuming that, 
we made a least squares fit of the bits needed to the formula $B = an\log(n)$ which 
yielded the approximate coefficient value $a = 38$.
Another least squares fit to the formula $B = an + bn\log(n)$, yielded the coefficients
$a = ?$ and $b = ?$, giving greater emphasis to the linear term. Both fits were reasonably 
good with fairly small variance in the ratios of actual bits to bits predicted by the fit.
But a fit to the formula $B = an$ had greater variance.
\begin{verbatim}
-----
figure:
x = log n vs y = log t 
point symbols for B (bits), for various n.
lines for B = a n lg n and for  B = a n + b n lg n 
-----
\end{verbatim}

These formulas predict ? or ? bits for the $M_c$ on the largest model in consideration,
the case $n = 7168$.  If this range is approximately right the run time
will be ? to ?.   Since the computation is embarrassingly parallel - independent computations 
mod each prime - we propose to 
do it using, perhaps,  x processors.  After efforts to improve cost per bit for BBSM,
We plan to carry out this computation, perhaps wthin a month or two of 
this writing.

\subsection{decreasing cost per bit in the blackbox approach}


combining diagonal factors.

Optimizing prime length

floating point and other things tried that were not much help.


\section{Conclusion} %section 6
Way A is best for large dimension, way B is best for small.
The crossover is at dimension = X.
Parallelism is important to get the job done.
We can to do signature of a dense $n\times n$ integer matrix having $\log(n)$ bit entries 
in a minute if $n \leq ?$, a day if $n \leq ?$ and x in a week, but not beyond that 
due to memory limitations.  By blackbox method we can do $n \leq ?$ in a year.

The world needs a supercomputing center for algebraic computation!
..and so on.

Open questions:
A good blackbox (memory efficient) method is desirable for psd.
A faster (preferably also memory efficient) method is desirable for signature.
These methods apply to Lyapunov stability computation(?!).


%\renewcommand{\refname}{\vspace{-.3in}}% suppresses \section*{References}
\bibliographystyle{plainnat}
\bibliography{strings,crossrefs,saunders,new}

%crossrefs.bib  itr2K.bib     new.bib          strings.bib        xrefs.bib
%eccad03.bib    kaltofen.bib  rank-certif.bib  stuff.bib
%issac.bib      meyer.bib     saunders.bib     valencebiblio.bib

\end{document}
