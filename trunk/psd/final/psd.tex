\documentclass{acm_proc_article-sp}
%\documentclass[final]{proceedings}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{multicol}
%\usepackage{subeqn}
\usepackage{url}

%%%%%%%%%%%%
% material borrowed form nsf/linalg05 in futile attempt to define \newblock
%\documentclass[12pt]{article}
%\usepackage{nsf} %%% Erich's NSF proposal style (itr/nsf.sty)
%\usepackage{mathptm} %%% postscript times; does not screw up pdf formats
%\usepackage{newlfont}
%\usepackage{xspace}
%\usepackage{array}
%\usepackage{amsmath}
%\usepackage{amsfonts}
%\usepackage{longtable}
%\usepackage{rotating}
%%%%%%%%%%%%

%\usepackage{crop}
%\crop
%\makeindex

%\usepackage{natbib}
%\bibpunct[,]{[}{]}{,}{a}{}{,}

\input{defs}

%\advance\topmargin by +0.2in

\begin{document}

%
% --- Author Metadata here ---
\conferenceinfo{ISSAC}{'05, KLMM, Chinese Academy of Sciences, Beijing, China}
%\setpagenumber{50}
%\CopyrightYear{2005} % Allows default copyright year (2002) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (X-XXXXX-XX-X/XX/XX) to be over-ridden.
% --- End of Author Metadata ---

\title{Signature of Symmetric Rational Matrices and the Unitary Dual of Lie Groups
%\titlenote{A full version of this paper is available as ...
%
}
\subtitle{[Extended Abstract]}

% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
%% e-mail address with \email.
\numberofauthors{3}
\author{
\alignauthor Jeffrey Adams\titlenote
{Supported by NSF grant DMS 0200851.}\\
       \affaddr{Department of Mathematics}\\
       \affaddr{University of Maryland}\\
       \affaddr{College Park, MD, 20742 USA}\\
       \email{jda@math.umd.edu}
\alignauthor B. David Saunders\titlenote
{Supported by NSF grants CCR 0098284 and 0112807.}\\
       \affaddr{Department of Computer and Information Sciences}\\
       \affaddr{University of Delaware}\\
       \affaddr{Newark, DE, 19716 USA}\\
       \email{saunders@cis.udel.edu}
\alignauthor Zhendong Wan{\raisebox{9pt}{\dag}}
%\titlenote{Supported by NSF grants CCR 0098284 and 0112807.}\\
       \affaddr{Department of Computer and Information Sciences}\\
       \affaddr{University of Delaware}\\
       \affaddr{Newark, DE, 19716 USA}\\
       \email{wan@cis.udel.edu}
}
\date{14 January 2005}
\maketitle
\begin{abstract}
\input{abstract}
\end{abstract}

% A category with the (minimum) three required fields
\category{G.4}{Mathematical Software}{Algorithm Design and Analysis}
%A category including the fourth, optional field follows...
%\category{J.2}{Physical Sciences and Engineering}[Mathematics]
\category{F.2.2}{Analysis of Algorithms and Problem Complexity}
{Nonnumerical Algorithms and Problems}[Computations on discrete structures]

\terms{Algorithms, Performance}

\keywords{
matrix signature, symmetric matrix, {L}ie group}
% NOT required for Proceedings

\section{Introduction}
%In the study of Lie group representations it is helpful to determine key
%properties of certain operators, which in the matrix representations are
%dense matrices with integer or rational number entries.  At issue is to compute
%which of these are positive definite, semi-definite or have other pattern of
%eigenvalue signs.

We propose, analyze, and test several algorithms for 
computing the signature and for verifying or disproving specific
definiteness properties of symmetric integer matrices.  
The latter is sometimes easier than the signature.  
Also there is often a considerable difference in the 
cost of verifying (certifying) a property and of certifying it's negation.
This is primarily an experimental paper whose purpose is to assess the feasibility 
of exact integer linear algebra methods for signature and definiteness determinations
that are useful in the study of Lie group representations.
The combination of measurements and analysis of asymptotic growth rates of time
and memory use that we provide is for the purpose of predicting the cost of the larger
computations of interest, so as to determine the most promising algorithms and the 
hardware resource needs.
The algorithmic asymptotic complexities reported here are largely straightforward 
applications of known results. 
We have an interesting observation which
allows both efficient binary tree structured Chinese remaindering to be used to construct 
numbers moduli a bunch of distinct primes and an early termination strategy at the same time.
Practical and asymptotic speed up is given when using early termination.
We have measured the time and memory costs of three algorithms on some matrices arising
in group representations, 
studied their relative merits, and developed formulas to predict the costs for large matrices.
We remark that the largest computation we've succeeded in doing with
a standard package such as Mathematica is for a matrix of dimension
400, whereas the largest needed is for dimension 7168  Our measurements 
indicate it will be feasible.

In section 2, the motivation for this work in the study of Lie group
representations is presented.  This application creates large problems
straining our limits of time and memory.
In section 3 we present the proposed algorithms and discuss their mathematical
basis and complexity.
Also included is some discussion
of unimplemented alternatives and of the space issues for large instances
of the problems. In section 4, we apply these algorithms to the matrices from 
Lie group representations.
Finally, in section 5, experimental results are reported
and, in section 6, conclusions are drawn on the state of these problems.

\section{Lie Group Representations and Unitary Duality}
\input{liegroups} % Jeff


\section{Signature Algorithms} %section 3

For the rest of this paper we will consider the question of signatures and
sign patterns
of symmetric matrices in general, but constantly keeping in mind the 
operators generated from Lie group matrix representations.
The signature of a real symmetric matrix $A$ is 
generally defined as $\sigma = \p - \n$, the number by which positive eigenvalues 
outnumber negative ones.  
%For our purposes, 
Define the {\em signature triple} to be the 
$\signature(A) = (\p, \z, \n)$, 
where $\p$ is the number of positive eigenvalues, $\z$ is the 
multiplicity of zero as an eigenvalue, and $\n$ is the number of negative eigenvalues.
A real symmetric matrix is 
positive definite if $z$ and $n$ are zero and 
positive semi-definite if $n$ is zero.

For a polynomial with only real roots, Descartes' rule of signs can be used to 
determine the number of positive roots and the number of negative roots.
Thus the signature triple of a matrix can be determined from the signs in 
the vector of coefficients of the characteristic polynomial.  
It is computationally useful that,
when only the zero eigenvalue has multiplicity greater
than 1, the characteristic polynomial is just a shift of the minimal 
polynomial.  If $A$ has minimal polynomial $m(x)$ of degree $d$, we define
the shifted minimal polynomial to be $x^{n-d} m(x)$.  
Less well known
is that signature can also be determined in certain cases, in a similar way, 
from the vector consisting of the leading principal minors of $A$. 
%We call such vectors $\signature$-revealing vectors for the matrix.
In view of these connections, for a vector the {\em signature triple} 
is 
$\signature(v) = (\p, \z, \n)$, where
$\p$ is the number of alternating successive nonzero pairs, and 
$\n$ is the number of constant successive nonzero pairs,
$\z$ is the number of zeros at the end of the vector
(which corresponds to the multiplicity of zero as an eigenvalue in our case).
To be precise, 
a pair of entries, $(v_i, v_j)$ of $(v_0, v_1, \ldots v_n)$ is {\em successive} 
if $i < j$ and the entries between them are zero.
A successive pair of nonzero entries is {\em alternating}({\em constant}) if 
their signs are opposite(same).
%write a degree $n$ polynomial (such as the characteristic polynomial of $A$)
%as $f(x)$ as $\sum_{i=0}^n f_i x^{n-i}$, with corresponding coefficient
%vector $(f_0, f_1, \ldots, f_n)$.
The vector in question will be either the coefficient vector of a polynomial or 
$w = (w_0 = 1, w_1, \ldots, w_n)$, where $w_i$ is $(-1)^i m_i$, with $m_i$ being
the $i\times i$ leading principal minor of the given matrix. 
Following \cite{KaLo96:issac},
say a matrix of rank $r$ has 
{\em generic rank profile} if the leading principal minors 
of dimensions 1 through $r$ are nonzero. 

\begin{theorem} [Signature theorem]{\label{theorem:sig}}
Let $A$ be a real symmetric $n\times n$ matrix.
The following hold:
\begin{enumerate}
\item 
Signature is invariant under congruence, that is, if 
$Q$ is nonsingular then $\signature(A) = \signature(QAQ^T)$.
\item
$\signature(A) = \signature(\charpoly{A})$.
Also, if the nonzero eigenvalues of $A$ are distinct,
$\signature(A) = \signature(\shiftminpoly{A})$.
\item
If $A$ is in generic rank profile with rank $r$, 
$$\signature(A) = \signature(1, -m_1, \ldots, (-1)^r m_r, 0, \ldots, 0),$$
where $m_i$ is the $i$th leading principal minor of $A$, and the last $(n-r)$
entries are zero.
\item
A matrix in generic rank profile has a unique $A = LDL^T$ decomposition with 
unit lower triangular $L$ and diagonal $D = \diag(d_1, \ldots, d_n)$.  If $A$
has generic rank profile and rank $r$, then (ending with $(n-r)$ zeroes)\\
$\signature(A) = 
\signature(1, -d_1, d_1 d_2, \ldots, (-1)^r\prod_{1 \leq i \leq r}d_i, 0, \ldots, 0)$. 
%\signature(0, \ldots, 0, (-1)^r\prod_{1 \leq i \leq r}d_i, \rdots, d_2 d_1, -d_1, 1)$. 
\end{enumerate}
\end{theorem}

Proof.  A good source for these fundamental facts is \cite{gantmacher}.
In particular the third statement is a theorem of Jacobi, \cite[Chapter X, \S 3, theorem 2]{gantmacher}.
The fourth item follows since $d_i = m_i/m_{i-1}$.
See \cite[Chapter 4]{GoLo96} for a good discussion of $LDL^T$ decomposition.
\QED

The generic rank profile condition assures that the $m_i$ consist of nonzero entries followed
by zeros with no intermingling of zero and nonzero values, and the same applies to the 
diagonal D of the $LDL^T$ decomposition.
Interestingly, again see \cite{gantmacher}, the 
signature can be recovered even when there are some scattered zeroes among the nonzero 
leading minors, hence something less than generic rank profile is needed.  
We will not pursue this point further. 

\input{cra.tex}

The following theorem gives additional flexibility in using the strategies
for signature computation made available by {\tt GenCRA}.
\begin{theorem} {\label{theorem:random}}
Let $A$ be an $n\times n$ real symmetric matrix and let S be a set of nonzero
integers of sufficient size that $\epsilon = n^2 \log(n)^2/\abs{S}$ is as small as
desired.
\begin{enumerate}
\item
Let $D$ be a diagonal matrix whose $n$ diagonal entries are chosen
uniformly at random from $S$, 
and let $B = DAD^T$.  Then $$\Prob(\shiftminpoly{B} = \charpoly{B} \geq 1 - \epsilon.$$
\item
Let $Q$ be a butterfly matrix whose $n \log (n)$ defining entries are chosen
uniformly at random from $S$, 
and let $B=QAQ^T$.
Then $$\Prob(B \mbox{ is in generic rank profile}) \geq 1 - \epsilon.$$
\end{enumerate}
\end{theorem}

Proof:  These preconditionings are discussed in detail in \cite{CEKSTV02} \QED

For the generic rank profile condition, The butterfly is chosen as preconditioner
because the specified matrix $QAQ^T$ can be computed in $\softO(n^2)$ time.  
For our purposes here we can afford preconditioning complexity up to $\softO(n^3)$ time.
A general random matrix could be used for the preconditioner, or Toeplitz \cite{KS91} or sparse
preconditioners \cite[Section 6]{CEKSTV02}.
It is of interest to keep the size of the entries of the resulting matrix as small as
possible.

We build three algorithms on the theory described above, two using the minimal polynomial (for blackbox
and for dense matrices) and one using $LDL^T$ decomposition.

\begin{algorithm} {BBSM [BlackBox Signature by Minpoly]\label{bbsm}}
\\Input:  $A$, a symmetric matrix in blackbox form,\\
$S$, a set of integers from which to make random selections.
\\Output: The signature $\signature(A) = (p, z, n)$.
\Stmt[1.] $[$ Preconditioning may be necessary $]$
Let $q$ be a random prime.\\ 
Let $r := \rank{A,q}$ and $m_q = \minpoly{A, q}$.\\ 
%$[$ \rank{} and \minpoly{} can work even if $A$ is a rational matrix, 
%provided $q$ does not divide any denominator of $A$.$]$\\
If $\deg(m_q) < n \mbox{ and } \deg(m_q) \leq r$, 
let $B := DAD$ [A blackbox],
for a random diagonal matrix $D$ with entries chosen 
from $S$,\\
Otherwise, let $B := A$.\\
$[$ Now $B$ has the same signature as $A$ and its charpoly is 
its shifted minpoly, with high probability. ]
\Stmt[2.] Choose a set $\set$ of primes, and the sample size $\rSize$, such that
the error probability is as small as desired.
Return \signature($x^{\max(0, n-r-1)}$GenCRA$(B, \minpoly{}, \set, \rSize)$).

\end{algorithm} 

The $r = \rank{A, p}$ and $v = \minpoly{A, p}$ algorithms 
used to compute the rank and the minimal polynomial 
of $A$ mod $p$ respectively
are as in \cite{Wie86, KS91, CEKSTV02}, for example.  
Here \minpoly{} and \rank{} run in time $\softO(nE)$
and are probabilistic with probability
of error no more than $1/p$,
where $E$ is the cost of a single matrix-by-vector product.
But \rank{} will never return a value
greater than the true rank and \minpoly{} always computes at least a factor
of the true minimal polynomial of $A$ mod $p$. 
%It will returns true or false dependent on the degree of the minimal polynomial.
%(This is in addition to the possibility of a bad prime $p$.) 
%minpoly).  In any case the shift, \shiftminpoly{}, meets the requirements of {\tt image} for GenCRA.
$DAD$ is the blackbox whose matrix-vector product is formed as $y = D(A(Dx))$.
%Optimalization may be done for $DAD$.
%However, if $A$ itself is represented as an unexpanded product of sparse matrices, as in 
%\M{\nu}{\sigma},
%some time can be saved by explicit multiplication of $D$ into the first and last factors. 

\begin{algorithm} {DSM [Dense Signature by Minpoly] \label{dsm}}
\\Input: Matrix $A$, in dense form.
\\Output: $\signature(A)$.
\Procspec \\
Apply algorithm {\tt BBSM}, except use 
$r = \rank{A, p}$ \cite{DumasGiorgiPernet:2004:issac} and 
\\$v = \minpoly{B, p}$, \cite{Pernet03}, % wan pernet
algorithms which
are 
available for the explicit (dense) matrix representation.
Then $\rank{}$ and $\minpoly{}$ are deterministic eliminations running in time $\softO(n^3)$
and using O($n^2 d$) memory 
Of course, in this case the DAD preconditioning is done 
explicitly (and cheaply).
Again, of course, $p$ may be a bad prime (minpoly of A mod p not equal mod p 
image of integer minpoly of A.. 
\end{algorithm} 

Especially for blackbox matrices, it is useful that minpoly computation can suffice,
because we have faster algorithms for minpoly than for charpoly in that case.
But for the DSM algorithm above, an alternative is to use the charpoly() function as the $\reveal$ function, see \cite{Pernet03} for example.
% also cite keller-gehrig?
Also note that the minpoly suffices for determining the sign pattern (not full signature)
even without preconditioning.  
%To be precise, the sign pattern of $A$ is determined from 
%$s = \signature(\mbox{minpoly of A})$ 
%by replacing positive entries in $s$ by $+$, though the point is that
%the pattern may sometimes be more easily determined than the signature itself.
%In particular, 
When the minpoly is of low degree or has small coefficients, this is a great savings.
In general, though, BBSM is not a fully memory efficient algorithm because of the size of the 
\signature-revealing vector.  It is possible that the technique of \cite{BEPP97} could
be used to determine the signs using less memory and perhaps less time.  This approach
deserves further examination.

%However,
%it is not necessary to compute the characteristic or minimal 
%polynomial to answer signature and definiteness questions.  
Alternatively, 
combining Theorem \ref{theorem:sig}, part 3, 4, 
and theorem \ref{theorem:random},
part 2,  we may work from 
the $LDL^T$ decomposition of a matrix in generic rank profile.  
If the matrix should fail to have generic rank profile, this will be detected during
the eliminations because of the need for pivoting.
We use preconditioners $Q$ to assure that $QAQ^T$ has generic
rank profile.
In some cases symmetric pivoting could be used instead to avoid the 
increased entry size caused by preconditioning.  

The $LDL^T$ decomposition based signature algorithm assumes a procedure 
$\lpm(v, A, p)$ which 
computes the vector 
%$$((-i)^k\prod_{1\leq i \leq k}D_{i,i}, \cdots, D_{1,1}D_{2,2}, -D_{1,1}, 1),$$
%$$(1, -d_1, d_1 d_2, \ldots, (-1)^r\prod_{1 \leq i \leq k}d_i)$$ 
of leading principal minors with alternating signs as described above, 
up to but not including the first which is zero mod $p$.  
The matrix mod $p$ can fail to have generic rank profile.
We could modify the $LDL^T$ to successfully handle such cases, but it is a 
remote possibility and not worth the overhead.  
We simply reject such primes as bad primes.
The procedure $\lpm$ 
meets the requirements of a \signature-revealing function for GenCRA.  

\begin{algorithm}{DSLU}\label{siglu}\\
$[$Dense matrix Signature by LU-decomposition]\\
Input: $A$, an integer symmetric $k\times k$ matrix.\\
Output: Signature $\signature(A) = (\p, \z, \n)$.
\Stmt[1.] $[$ Preconditioning may be necessary $]$\\
Let $q$ be a random prime.\\ 
Let $\lpm(v_q, A,q)$, $r := \rank{A, q}$.\\
%$[$ \lpm can work even if $A$ is a rational matrix, 
%provided $q$ does not divide any denominator of $A$.$]$\\
If length of $v_q$ less than $r + 1$, let $B := QAQ^T$,\\
$~~~$ for a random integer matrix $Q$ with entries chosen\\ 
$~~~$ from $[1, \cdots, s]$ (or any set of $s$ nonzero integers).\\
$~~~$ [Q may be a Toeplitz or Butterfly matrix for speed]\\
Otherwise, let $B := A$.\\
$[$ Now B has the same signature as A and has generic rank profile. ]\\
Choose a set $\set$ of primes, and the sample size $\rSize$, such that
the error probability is as small as desired.
\Stmt[2.] %%$[$ Lift $]$\\
Return \signature($x^{n-r}$GenCRA$(B, \lpm(), \set, \rSize)$).
\end{algorithm}
%Cholesky mod primes?
\begin{theorem}
Let $A$ be an integer blackbox $\n \times n$ symmetric matrix,
whose matrix-vector product costs $e$ operations and 
whose entries would be of bit length at most $d$ if they were explicitly constructed.

Algorithms BBSM, DSM, and DSLU are Las Vegas probabilistic algorithm 
if the Hadamard bound is used and are
Monte Carlo if early termination is used in the remaindering.
Even with early termination, DSM is Las Vegas if the computed 
integer minimal polynomial is checked by evaluation at $A$ over the integers. 
This costs $\softO(n^4)$.% Zhendong check it.
Also a minimal polynomial verification by application to the identity 
could be done after
BBSM making it Las Vegas for the sign pattern of the signature.

Let $h$ be a bound for the bit lengths of the values constructed using GenCRA, 
(the length of the largest characteristic polynomial coefficient or of the 
largest leading principal minor).  Then the expected run times are in 
$\softO(neh)$ for BBSM and in $\softO(n^3h)$  for DSM and DSLU.
By the Hadamard bound, $h$ is in $\softO(nd)$, so the expected run times
are also in $\softO(n^2 e d)$ for BBSM and in 
$\softO(n^4d)$ for DSM and DSLU.

In particular if $e \in \softO(n)$, and $d \in \bO(\log(n))$,
then the BBSM expected run time is $\softO(n^3)$ and the DSM, DSLU expected times are in $\softO(n^4)$.
\QED
\end{theorem}

Also, with any of the \signature-revealing vectors, the entries tend to grow 
in proportion to their index.  In particular the $i$th entry is either an $i\times i$
minor or sum of $i\times i$ minors, so is bounded by the Hadamard bound which is in $\softO(id)$.
A heuristic to determine indefiniteness 
computes the first few vector entries, using many fewer remaindering steps than are required
for the later entries.  If the sign pattern fails to be constant or strictly alternating
the matrix is indefinite.  It is an open question whether a conjugacy class preconditioning,
$A \rightarrow QAQ^T$, could make probable that early entries indicate definiteness (cheaply).

\input{application}
%\section{Checking the Model and Operator}

\section{Experiments} %section 5
Our goal is to know which algorithm to use and how much resources it will take 
for computing the signature of %the $\M{\nu}{n} = \M{\nu}{\rho}$, 
the $\M{\nu}{\rho}$ in equation 3,
the matrix generated by the facet $\nu$
for the model of dimension $n$ of the group \EE.  
In particular, we focus on the case when
the facet is the challenge facet $\nu_0 = 1/40 (0,1,2,3,4,5,6,23)$, 
and the goal is to verify (or disprove) that $\M{\nu_0}{n}$ is positive definite.  
To emphasize the model dimension $n$ in what follows, let $J_n = \M{\nu_0}{\rho}$.

The three signature algorithms we have discussed are BBSM, DSM, DSLU.
We have computed full solutions by each model for the models
Q2, $\ldots$, Q12, Q22, Q32, Q42, Q52(dimension 1008),
and we have computed single modular step costs for every 10th model on up to the largest,
Q112(dimension 7168).
The experiments involved individual run times up to about one day and 
verified, where the full computation was done, that $J_n$ is indeed positive definite.
All computations reported here were performed on a 3.2GHz Xeon processor with 6GB of main memory.
%and xx L2 cache.
Our purpose here is to determine the algorithm to use and resources needed for the full 
signature computation on examples of all dimensions (up to 7168) and to have tools to 
estimate the cost for matrices defined by other facets.

The blackbox representation of $J_n$ is a product of very sparse factors. 
%Since the cost of computing the characteristic polynomial depends on 
%the cost of applying $A$ to a vector, 
%we examined the number of nonzero elements in total in these factors.
Since the blackbox method cost is sensitive to $e$, the cost of the single matrix-by-vector product, 
we examined the number of nonzero elements in total in these factors.
To guess the expected growth of $e$ with dimension, we measured the total number
of nonzero entries in the factors of every 10th model up to Q112.
Since the number of factors is fixed and they are extremely sparse we expect a linear or 
$n\log(n)$ term to dominate.  The formula 
$e = 132.5900691 n + 13.12471811 n \ln(n)$, a least squares fit to the data, is 
plotted in figure 1 along with the data values. 
The fit is extremely
good and we see that $e$ is only slightly super-linear.  Therefore we expect
the runtime of characteristic polynomial computation mod a single prime to grow at 
a rate only slightly above quadratic in $n$.
\begin{figure}[h] %1
\caption{Blackbox number of non-zeroes}
\epsfig{file=../graph/nonzero.eps,  
height=1.4in, width=.45\textwidth}  
\end{figure}

Each algorithm involves a series of steps mod primes $p_i$, each step being
a computation of \signature-revealing vector (a minpoly or diagonal of $LDL^T$-decomposition) 
mod $p_i$.
The computation is finished when the CRA remaindering has given an image of the vector
with modulus, \(M = 2 \prod p_i\), sufficiently large to recover the actual (signed) 
vector entries.  If we know a bound $d$ for the length of the entries of $J_n$,
we have by the Hadamard bound a maximal length $b = n(\log(n)/2 + d) + 1$ for $M$.

The dense matrices are computed by applying the black box to the columns of the 
identity matrix (over $\Z$, not modularly), and so the computation is 
sensitive to the length of the 
actual integer entries.
%of the expanded (and dense) matrix.
Let $d$ be the bit length of the 
largest entry.  
We do not have a theory to predict how matrix entry bit length $d$ may depend on the 
model dimension for a fixed facet. 
We have plotted the bit length $d$ of the largest entry of $J_n$ along with
the fitted curve $d = 67.7 + 33.2\log(n)$ in figure 2.  
One sees the $\log(n)$ playing a stronger role than that for the non-zeroes in the blackbox,
but the fit to the data is poorer and likely to have less predictive value.
We noted that the $n^2$ entries of $A$ all have about the same length, so $n^2 d$ accurately
describes the storage required for $A$.
\begin{figure}[h] %2
\caption{Bit length of entries}
%Dense matrix bits per entry}
\epsfig{file=../graph/bits.eps, 
height=1.4in, width=.45\textwidth}  
\end{figure}

It will also be useful to use $d$ in estimating the number of remaindering steps.
The needed bit length $b$ for the modulus $M$ is at most 1 bit (for sign) more 
than the length of the Hadamard bound.  The rows norms are no more than \(\sqrt{n(e^d)^2}\),
so their product has length bounded by $n(\log(n)/2 + d)$.
Reasonably consistent with this prediction is the number of bits used when we ran
the full algorithms on $J_n$ for $n$ up to 1008 (model Q52).  
The curve in figure 3 is a least squares fit,
$b = 150.1 n + 17.30n ln(n)$,
The number of bits for the charpoly coefficients (sums of minors) are expected 
to be slightly larger than for the entries of the  leading principal minors vector,
but most likely, the final term, determinant of $J_n$, is dominant.  
The number of bits needed appears to be slightly super-linear, not quite as large as
the worst case Hadamard bound level.  
\begin{figure}[h] %3
\caption{Bit length of final modulus}
\epsfig{file=../graph/bit_required.eps, 
height=1.4in, width=.45\textwidth}  
\end{figure}

Next we consider the costs of the modular steps.
Let \step(s) denote the cost per step which varies with length, $s$, of the prime, 
but is essentially constant among primes of the same length.
The total time  is $b/s \times \step(s)$ when using $s$ bit primes. 
Thus we see the advantage of minimizing $\step(s)/s$, the cost per bit.
After discussing $\step(s)/s$ and determining it's value, for each algorithm,  
we will know how to estimate the full runtime of the algorithms.

We expect to be able to reduce the LU step cost
by half, as an elimination better tailored to symmetric matrices can be done, while still taking
advantage of block operations using BLAS. For the blackbox algorithm, block methods may 
be able to reduce the costs somewhat, and the DSM may perform relatively better than
here if the dense \charpoly{} algorithm \cite{Pernet03} % the KG based idea
is used rather than preconditioning.
At any rate, in figure 4 we see the performance of the modular steps of the three algorithms,
BBSM, DAME, DSLU as currently implemented.
Asymptotically, the BBSM step runtime, $s$, is expected to grow in
proportion to $ne$. There was a good fit to $s = 61 n^2\log(n)$ nanoseconds per bit
(recall that $e$ has a $n\log(n)$ dominant term).
The algorithms DSM and DSLU depend on the construction of the full integer matrix of 
$J_n$, which is done once.  The step times consist of computing a modular
image of that and proceeding with an elimination on the image.
The implementation of the Krylov matrix construction in the \minpoly{} algorithm of $DSM$,
uses a technique of recursive doubling to better exploit fast matrix multiplication,
and gets a step time $s$ which fits the formula 
$t = 1.8   n^3  + 0.16  n^3 \log(n)$ nanoseconds per bit.
$t = 0.37 n^3 \log(n)$ nanoseconds per bit.
quite well.  
The DSLU step fits 
$t = 0.3838888362 n^3$ nanosec/bit.
But these formulas have to be worked to the bit cost level!!!

The log factor for DSM step can be removed, so it remains possible that it can become 
competitive or participate with BBSM in a heterogeneous distributed computation, running
on the machines with larger memory.  
The time formula for the dense matrices should remain valid as long as the modular
step fits in main memory. After than swapping would dramatically increase times.
Assuming a memory capable of holding a 2GB virtual memory,
this would allow for $n^2$ words, so $n < 2^{15}$.  For all practical purposes,
the blackbox step has no memory limitation.
\begin{figure}[h]
\caption{Time per bit of modulus}
\epsfig{file=../graph/steps.eps,
height=1.4in, width=.45\textwidth}  
\end{figure}


The total runtime then involves the time per bit in the modular steps times the number
of bits in the \signature-revealing vector.  The Chinese remaindering adds a cost that
is similar for each but large enough to mute somewhat the effect of the differing step costs.
However with the early termination technique of the previous section this remaindering
is a smaller factor than in earlier timings.  
%For instance, without the asymptotic 
%improvement in the termination strategy our time for x was y, now it is z.
The two dense algorithms also incur a lower order
cost for the creation of the expanded integer matrix initially.
In figure 5 we show the overall run times (where the computations have been
done so far) together with their least squares fit formulas.
The formulas for time $t$ in nanoseconds are 
BBSM: $t = 10050 n^3 \log(n)$, % blackbox full time formula
DSM: $t = 6.08 n^4 \log(n)$, % blas full time formula
and DSLU: $t = 1.45 n^4 \log(n)$.% lu full time
%$t = 2240000 n^2  + 66800 n^3$ % blackbox full time formula
%$t = 12900 n^3  + 28.9 n^4$ % blas full time formula
%$t = 9650 n^3  + 0.222 n^4$ % lu full time
Theory predicts a second log factor in the BBSM time, coming from the 
the single matrix-by-vector product cost, $e$.
However that effect is weak and the fit was poorer.
\begin{figure}[h]
\caption{Total run time}
\epsfig{file=../graph/time.eps,
height=1.4in, width=.45\textwidth}  
\end{figure}

The memory needed to store the \signature-revealing vector mutes the memory 
advantage of BBSM.  However the modular images of the vector are easily stored on hard disk
and manipulated from there, so the memory advantage remains real.
The crossover point of the runtime formulas for BBSM and DSLU is around $n = 6931$. 
However the formula $t = 10.02 n^4$ fits the DSLU data about as well as our formula 
above.  Using that form, the crossover point is at $n = 9150$.

%$t = 0.002240174633 n^2  + 0.00006676817979 n^3$ % blackbox full time formula
%$t = 0.00001285796899 n^3  + 0.2885270500 10^{-7} n^4$ % blas full time formula
%$t = 0.9645194584 10^{-5}   n^3  + 0.2217193167 10^{-9}   n^4$ % lu full time

%\begin{figure}
%\caption{predict}
%\epsfig{file=../graph/predict.eps,
%height=1.2in, width=.45\textwidth}  
%\end{figure}


%Optimizing prime length

%floating point and other things tried that were not much help.
%fixme talk floating point in alg section

%\input{lowrank}

\section{Conclusion} %section 6
We have demonstrated that we can compute (on current hardware) the signature 
of a dense $n\times n$ integer matrix having entries of bit length around $\log(n)$ 
in a minute if $n \leq 200$, in three hours if $n = 1000$ and (projected) in a CPU year 
for $J_{7168}$.  Beyond that size, using algorithm BBSM, the time 
grows at a rate slightly higher than $n^3$ and memory is not a constraint 
(except for storage of the sparse factors of the blackbox).
However, we conclude that algorithm DSLU serves best for matrices of dimension $n < 7000$. 
It is tossup between DSLU and BBSM for dimensions $7000 \leq n \leq 9000$ and BBSM is
superior beyond that.  
DSLU time grows slightly above $n^4$.

For $J_{7168}$, DSLU requires explicit use of file storage of the expanded matrix
and all algorithms should do this for intermediate results 
(modular images of \signature-revealing vectors) because of the large size.
We have not measured the cost of this file manipulation.
At the crossover about one CPU year is required and DSLU needs a large memory.
The run time is expected to be about a CPU year, so 
parallel computation is desirable 
(and is quite straightforward for either algorithm)
on distributed or shared memory hardware.

It is an open question whether definiteness can be determined fundamentally faster than
signature.  There is a fast Monte Carlo algorithm for rank, hence for distinguishing
semi-definite from definite matrices.  We have sketched a heuristic that sometimes 
can determine indefiniteness much faster than the signature computation.

To provide for the needs of Lie group representation studies, both BBSM and DSLU will 
be further refined and their parallel implementation developed.  Also the judicious 
incorporation of numeric computation is a possibility.


%\renewcommand{\refname}{\vspace{-.3in}}% suppresses \section*{References}
%\fontsize{6pt}{7pt}
\bibliographystyle{plain}
\bibliography{../strings,../crossrefs,../saunders,../new}

%crossrefs.bib  itr2K.bib     new.bib          strings.bib        xrefs.bib
%eccad03.bib    kaltofen.bib  rank-certif.bib  stuff.bib
%issac.bib      meyer.bib     saunders.bib     valencebiblio.bib

\end{document}
